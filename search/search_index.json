{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to CleanQRL","text":"<p>CleanQRL is a Reinforcement Learning library specifically tailored to the subbranch of Quantum Reinforcement Learning and is greatly inspired by the amazing work of CleanRL. Just as the classical analougue, we aim to provide high-quality single-file implementation with research-friendly features. The implementation follows mainly the ideas of CleanRL and is clean and simple, yet can scale nicely trough additional features such as ray tune. The main features of this repository are</p> <ul> <li>\ud83d\udcdc Single-file implementations of classical and quantum version of 5+ Reinforcement Learning agents </li> <li>\ud83d\udcbe Tuned and Benchmarked agents (with available configs)</li> <li>\ud83c\udfae Integration of gymnasium, mujoco and jumanji</li> <li>\ud83d\udcd8 Examples on how to enhance the standard QRL agents on a variety of games</li> <li>\ud83d\udcc8 Tensorboard Logging</li> <li>\ud83c\udf31 Local Reproducibility via Seeding</li> <li>\ud83e\uddeb Experiment Management with Weights and Biases</li> <li>\ud83d\udcca Easy and straight forward hyperparameter tuning with ray tune</li> </ul> <p>What we are missing compared to CleanRL:</p> <ul> <li>\ud83d\udcb8 Cloud Integration with docker and AWS </li> <li>\ud83d\udcf9 Videos of Gameplay Capturing</li> </ul> <p>You can read more about CleanRL in our upcoming paper and see the github wiki for additional documentation.</p> <p>For full documentation visit mkdocs.org.</p>"},{"location":"#plain-codeblock","title":"Plain codeblock","text":"<p>A plain codeblock:</p> <pre><code>Some code here\ndef myfunction()\n// some comment\n</code></pre>"},{"location":"#code-for-a-specific-language","title":"Code for a specific language","text":"<p>Some more code with the <code>py</code> at the start:</p> <pre><code>import tensorflow as tf\ndef whatever()\n</code></pre>"},{"location":"#with-a-title","title":"With a title","text":"bubble_sort.py<pre><code>def bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n</code></pre>"},{"location":"#with-line-numbers","title":"With line numbers","text":"<pre><code>def bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n</code></pre>"},{"location":"#highlighting-lines","title":"Highlighting lines","text":"<pre><code>def bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n</code></pre>"},{"location":"#icons-and-emojs","title":"Icons and Emojs","text":""},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"algorithms/","title":"Algorithms","text":""},{"location":"algorithms/#overview","title":"Overview","text":"Algorithm Variants Implemented \u2705 REINFORCE <code>reinforce_classical.py</code>, docs <code>reinforce_quantum.py</code>, docs \u2705 Deep Q-Learning (DQN) <code>dqn_classical.py</code>, docs <code>dqn_quantum.py</code>, docs \u2705 Proximal Policy Gradient (PPO) <code>ppo_classical.py</code>, docs <code>ppo_classical_continuous.py</code>, docs <code>ppo_classical_jumanji.py</code>, docs <code>ppo_quantum.py</code>, docs <code>ppo_quantum_continuous.py</code>, docs <code>ppo_quantum_jumanji.py</code>, docs \u2705 Deep Deterministic Policy Gradient (DDPG) <code>ddpg_classical.py</code>, docs <code>ddpg_quantum.py</code>, docs"},{"location":"algorithms/#reinforce","title":"REINFORCE","text":""},{"location":"algorithms/#reinforce_classicalpy","title":"<code>reinforce_classical.py</code>","text":"<p>The <code>reinforce_classical.py</code> has the following features:</p> <ul> <li>\u2705 Works with the Box observation space of low-level features</li> <li>\u2705 Works with the Discrete action space</li> <li>\u2705 Works with envs like CartPole-v1</li> <li>\u2705 Vectorized Environments </li> </ul>"},{"location":"algorithms/#implementation-details","title":"Implementation details","text":""},{"location":"algorithms/#experiment-results","title":"Experiment results","text":""},{"location":"algorithms/#reinforce_quantumpy","title":"<code>reinforce_quantum.py</code>","text":"<p>The <code>reinforce_quantum.py</code> has the following features:</p> <ul> <li>\u2705 Works with the Box observation space of low-level features</li> <li>\u2705 Works with the Discrete action space</li> <li>\u2705 Works with envs like CartPole-v1</li> <li>\u2705 Vectorized Environments </li> </ul>"},{"location":"algorithms/#implementation-details_1","title":"Implementation details","text":""},{"location":"algorithms/#experiment-results_1","title":"Experiment results","text":""},{"location":"algorithms/#dqn","title":"DQN","text":""},{"location":"algorithms/#dqn_classicalpy","title":"<code>dqn_classical.py</code>","text":"<p>The <code>dqn_classical.py</code> has the following features:</p> <ul> <li>\u2705 Works with the Box observation space of low-level features</li> <li>\u2705 Works with the Discrete action space</li> <li>\u2705 Works with envs like CartPole-v1</li> <li>\u274c Vectorized Environments not enabled</li> <li>\u274c Requieres <code>replay_buffer.py</code> (no single file implementation)</li> </ul>"},{"location":"algorithms/#implementation-details_2","title":"Implementation details","text":""},{"location":"algorithms/#experiment-results_2","title":"Experiment results","text":""},{"location":"algorithms/#dqn_quantumpy","title":"<code>dqn_quantum.py</code>","text":"<p>The <code>dqn_quantum.py</code> has the following features:</p> <ul> <li>\u2705 Works with the Box observation space of low-level features</li> <li>\u2705 Works with the Discrete action space</li> <li>\u2705 Works with envs like CartPole-v1</li> <li>\u274c Vectorized Environments not enabled</li> <li>\u274c Requieres <code>replay_buffer.py</code> (no single file implementation)</li> </ul>"},{"location":"algorithms/#implementation-details_3","title":"Implementation details","text":""},{"location":"algorithms/#experiment-results_3","title":"Experiment results","text":""},{"location":"benchmarks/","title":"Benchmarks","text":""},{"location":"examples/","title":"Tutorials","text":""},{"location":"get_started/","title":"Get Started","text":""},{"location":"get_started/#installation","title":"Installation","text":"<p>To run experiments locally, you need to clone the repository and install a python environment.</p> <pre><code>git clone https://github.com/georgkruse/cleanqrl.git\ncd cleanqrl\nconda env create -f environment.yaml\n</code></pre> <p>Thats it, now you set up!</p>"},{"location":"get_started/#run-first-experiments","title":"Run first experiments","text":"<p>Each agent can be run as a single file, either from the parent directory or directly in the subfolder. First, activate the environment <code>cleanqrl</code> and then execute the algorithm's python file:</p> <pre><code>conda activate cleanqrl\npython cleanrl/reinforce_quantum.py \n</code></pre> <p>or go directly into the folder and execute</p> <pre><code>conda activate cleanqrl\ncd cleanqrl \npython reinforce_quantum.py \n</code></pre> <p>Before you execute the files, customize the parameters in the  <code>Config</code> class at the end of each file. Every file has such a dataclass object and the algorithm is callable as a function which takes the config as input:</p> <pre><code>def reinforce_quantum(config):\n    num_envs = config[\"num_envs\"]\n    total_timesteps = config[\"total_timesteps\"]\n    env_id = config[\"env_id\"]\n    gamma = config[\"gamma\"]\n    lr_input_scaling = config[\"lr_input_scaling\"]\n    lr_weights = config[\"lr_weights\"]\n    lr_output_scaling = config[\"lr_output_scaling\"]\n    .... \n</code></pre> <p>This function can also be called from an external file (see below for details). But first, lets have a closer look to the the <code>Config</code>: </p> reinforce_quantum.py<pre><code>@dataclass\nclass Config:\n    # General parameters\n    trial_name: str = 'reinforce_quantum'  # Name of the trial\n    trial_path: str = 'logs'  # Path to save logs relative to the parent directory\n    wandb: bool = False # Use wandb to log experiment data \n\n    # Environment parameters\n    env_id: str = \"CartPole-v1\" # Environment ID\n\n    # Algorithm parameters\n    num_envs: int = 1  # Number of environments\n    total_timesteps: int = 100000  # Total number of timesteps\n    gamma: float = 0.99  # discount factor\n    lr_input_scaling: float = 0.01  # Learning rate for input scaling\n    lr_weights: float = 0.01  # Learning rate for variational parameters\n    lr_output_scaling: float = 0.01  # Learning rate for output scaling\n    cuda: bool = False  # Whether to use CUDA\n    num_qubits: int = 4  # Number of qubits\n    num_layers: int = 2  # Number of layers in the quantum circuit\n    device: str = \"default.qubit\"  # Quantum device\n    diff_method: str = \"backprop\"  # Differentiation method\n    save_model: bool = True # Save the model after the run\n</code></pre> <p>As you can see, the config is devided into 3 parts:</p> <ul> <li>General parameters: Here the name of your experiment as well as the logging path is defined. All metrics will be logged in a <code>result.json</code> file in the result folder which will have the time of the experiment execution as a prefix. You can also use wandb for enhanced metric logging. </li> <li>Environment parameters: This is in the simplest case just the string of the gym environment. For jumanji environments as well as for your custom environments, you can also specify additional parameters here (see #Tutorials for details).</li> <li>Algorithms parameters: All algorithms hyperparameters are specified here. For details on the parameters see the algorithms section</li> </ul> <p>Once you execute the file, it will create the subfolders and copy the config which is used for the experiment in the folder:</p> <pre><code>    config = vars(Config())\n\n    # Based on the current time, create a unique name for the experiment\n    config['trial_name'] = datetime.now().strftime(\"%Y-%m-%d--%H-%M-%S\") + '_' + config['trial_name']\n    config['path'] = os.path.join(Path(__file__).parent.parent, config['trial_path'], config['trial_name'])\n\n    # Create the directory and save a copy of the config file so that the experiment can be replicated\n    os.makedirs(os.path.dirname(config['path'] + '/'), exist_ok=True)\n    config_path = os.path.join(config['path'], 'config.yml')\n    with open(config_path, 'w') as file:\n        yaml.dump(config, file)\n\n    # Start the agent training \n    reinforce_quantum(config)   \n</code></pre> <p>After the execution, the experiment data is saved e.g. at: </p> <pre><code>...\nconfigs\nexamples\nlogs/\n    2025-03-04--14-59-32_reinforce_quantum          # The name of your experiment\n        config.yaml                                 # Config which was used to run this experiment\n        result.json                                 # Results of the experiment\n.gitignore\n...\n</code></pre> <p>You can also set the <code>wandb</code> variable to True: </p> <pre><code>@dataclass\nclass Config:\n    # General parameters\n    trial_name: str = 'reinforce_quantum_wandb'  # Name of the trial\n    trial_path: str = 'logs'  # Path to save logs relative to the parent directory\n    wandb: bool = True # Use wandb to log experiment data \n\n    # Environment parameters\n    env_id: str = \"CartPole-v1\" # Environment ID\n</code></pre> <p>This will create an additional folder for the wandb logging:</p> <pre><code>...\nconfigs\nexamples\nlogs/\n    2025-03-04--14-59-32_reinforce_quantum_wandb    # The name of your experiment\n        wandb                                       # Subfolder of the wandb logging\n        config.yaml                                 # Config which was used to run this experiment\n        result.json                                 # Results of the experiment\n.gitignore\n...\n</code></pre>"},{"location":"get_started/#run-experiments-via-config-files","title":"Run experiments via config files","text":"<p>Additionally, all algorithms can be executed using the <code>main functions</code> in the root directory. All parameters for the algorithms are specified in the <code>configs</code> folder. See additional information in the documentation.</p>"},{"location":"get_started/#experiment-logging","title":"Experiment Logging","text":"<p>By default, all metrics are logged to a json file. </p> <p>For tensorboard logging, you can use  ray tune (see more in the following section and the docs)</p> <p>To use experiment tracking with wandb, you need to set the boolean variable <code>wandb</code> in the config class or the config file to True and then run: </p> <pre><code>wandb login # only required for the first time\npython cleanrl/ppo_quantum.py \\\n</code></pre>"},{"location":"hyperparameter_tuning/","title":"Hyperparameter Tuning","text":""}]}