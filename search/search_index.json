{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-cleanqrl","title":"Welcome to CleanQRL","text":"<p>CleanQRL is a Reinforcement Learning library specifically tailored to the subbranch of Quantum Reinforcement Learning and is greatly inspired by the amazing work of CleanRL. Just as the classical analogue, we aim to provide high-quality single-file implementation with research-friendly features. The implementation follows mainly the ideas of CleanRL and is clean and simple, yet can scale nicely trough additional features such as ray tune. The main features of this repository are</p> <ul> <li>\ud83d\udcdc Single-file implementations of classical and quantum version of 5+ Reinforcement Learning agents </li> <li>\ud83d\udcbe Tuned and Benchmarked agents (with available configs)</li> <li>\ud83c\udfae Integration of gymnasium, mujoco and jumanji</li> <li>\ud83d\udcd8 Examples on how to enhance the standard QRL agents on a variety of games</li> <li>\ud83d\udcc8 Tensorboard Logging</li> <li>\ud83c\udf31 Local Reproducibility via Seeding</li> <li>\ud83e\uddeb Experiment Management with Weights and Biases</li> <li>\ud83d\udcca Easy and straight forward hyperparameter tuning with ray tune</li> </ul> <p>What we are missing compared to CleanRL:</p> <ul> <li>\ud83d\udcb8 Cloud Integration with docker and AWS </li> <li>\ud83d\udcf9 Videos of Gameplay Capturing</li> </ul> <p>You can read more about CleanQRL in our upcoming paper.</p>"},{"location":"#contact-and-community","title":"Contact and Community","text":"<p>We want to grow as a community, so posting Github Issues and PRs are very welcome! If you are missing and algorithms or have a specific problem to which you want to tailor your QRL algorithms but fail to do so, you can also create a feature request!</p>"},{"location":"#citing-cleanqrl","title":"Citing CleanQRL","text":"<p>If you use CleanQRL in your work, please cite our [paper]:</p>"},{"location":"#citing-cleanrl","title":"Citing CleanRL","text":"<p>If you used mainly the classical parts of our code in your work, please cite the original CleanRL paper:</p> <pre><code>@article{huang2022cleanrl,\n  author  = {Shengyi Huang and Rousslan Fernand Julien Dossa and Chang Ye and Jeff Braga and Dipam Chakraborty and Kinal Mehta and Jo\u00e3o G.M. Ara\u00fajo},\n  title   = {CleanRL: High-quality Single-file Implementations of Deep Reinforcement Learning Algorithms},\n  journal = {Journal of Machine Learning Research},\n  year    = {2022},\n  volume  = {23},\n  number  = {274},\n  pages   = {1--18},\n  url     = {http://jmlr.org/papers/v23/21-1342.html}\n}\n</code></pre>"},{"location":"get_started/","title":"Get Started","text":""},{"location":"get_started/#installation","title":"Installation","text":"<p>To run experiments locally, you need to clone the repository and install a python environment.</p> <pre><code>git clone https://github.com/georgkruse/cleanqrl.git\ncd cleanqrl\nconda env create -f environment.yaml\n</code></pre> <p>That's it, now you're set up!</p>"},{"location":"get_started/#run-first-experiments","title":"Run first experiments","text":"<p>Each agent can be run as a single file, either from the parent directory or directly in the subfolder. First, activate the environment <code>cleanqrl</code> and then execute the algorithm's python file:</p> <pre><code>conda activate cleanqrl\npython cleanrl/reinforce_quantum.py \n</code></pre> <p>or go directly into the folder and execute</p> <pre><code>conda activate cleanqrl\ncd cleanqrl \npython reinforce_quantum.py \n</code></pre> <p>Before you execute the files, customize the parameters in the  <code>Config</code> class at the end of each file. Every file has such a dataclass object and the algorithm is callable as a function which takes the config as input:</p> <pre><code>def reinforce_quantum(config):\n    num_envs = config[\"num_envs\"]\n    total_timesteps = config[\"total_timesteps\"]\n    env_id = config[\"env_id\"]\n    gamma = config[\"gamma\"]\n    lr_input_scaling = config[\"lr_input_scaling\"]\n    lr_weights = config[\"lr_weights\"]\n    lr_output_scaling = config[\"lr_output_scaling\"]\n    .... \n</code></pre> <p>This function can also be called from an external file (see below for details). But first, lets have a closer look to the the <code>Config</code>: </p> reinforce_quantum.py<pre><code>@dataclass\nclass Config:\n    # General parameters\n    trial_name: str = 'reinforce_quantum'  # Name of the trial\n    trial_path: str = 'logs'  # Path to save logs relative to the parent directory\n    wandb: bool = False # Use wandb to log experiment data \n    project_name: str = \"cleanqrl\"  # If wandb is used, name of the wandb-project\n\n    # Environment parameters\n    env_id: str = \"CartPole-v1\" # Environment ID\n\n    # Algorithm parameters\n    num_envs: int = 1  # Number of environments\n    total_timesteps: int = 100000  # Total number of timesteps\n    gamma: float = 0.99  # discount factor\n    lr_input_scaling: float = 0.01  # Learning rate for input scaling\n    lr_weights: float = 0.01  # Learning rate for variational parameters\n    lr_output_scaling: float = 0.01  # Learning rate for output scaling\n    cuda: bool = False  # Whether to use CUDA\n    num_qubits: int = 4  # Number of qubits\n    num_layers: int = 2  # Number of layers in the quantum circuit\n    device: str = \"default.qubit\"  # Quantum device\n    diff_method: str = \"backprop\"  # Differentiation method\n    save_model: bool = True # Save the model after the run\n</code></pre> <p>As you can see, the config is devided into 3 parts:</p> <ul> <li>General parameters: Here the name of your experiment as well as the logging path is defined. All metrics will be logged in a <code>result.json</code> file in the result folder which will have the time of the experiment execution as a prefix. You can also use wandb for enhanced metric logging. </li> <li>Environment parameters: This is in the simplest case just the string of the gym environment. For jumanji environments as well as for your custom environments, you can also specify additional parameters here (see #Tutorials for details).</li> <li>Algorithms parameters: All algorithms hyperparameters are specified here. For details on the parameters see the algorithms section</li> </ul> <p>Once you execute the file, it will create the subfolders and copy the config which is used for the experiment in the folder:</p> reinforce_quantum.py<pre><code>    config = vars(Config())\n\n    # Based on the current time, create a unique name for the experiment\n    config['trial_name'] = datetime.now().strftime(\"%Y-%m-%d--%H-%M-%S\") + '_' + config['trial_name']\n    config['path'] = os.path.join(Path(__file__).parent.parent, config['trial_path'], config['trial_name'])\n\n    # Create the directory and save a copy of the config file so that the experiment can be replicated\n    os.makedirs(os.path.dirname(config['path'] + '/'), exist_ok=True)\n    config_path = os.path.join(config['path'], 'config.yml')\n    with open(config_path, 'w') as file:\n        yaml.dump(config, file)\n\n    # Start the agent training \n    reinforce_quantum(config)   \n</code></pre> <p>After the execution, the experiment data is saved e.g. at: </p> <pre><code>...\nconfigs\nexamples\nlogs/\n    2025-03-04--14-59-32_reinforce_quantum          # The name of your experiment\n        config.yaml                                 # Config which was used to run this experiment\n        result.json                                 # Results of the experiment\n.gitignore\n...\n</code></pre> <p>You can also set the <code>wandb</code> variable to <code>True</code>:</p> reinforce_quantum.py<pre><code>@dataclass\nclass Config:\n    # General parameters\n    trial_name: str = 'reinforce_quantum_wandb'  # Name of the trial\n    trial_path: str = 'logs'  # Path to save logs relative to the parent directory\n    wandb: bool = True # Use wandb to log experiment data \n    project_name: str = \"cleanqrl\"  # If wandb is used, name of the wandb-project\n\n    # Environment parameters\n    env_id: str = \"CartPole-v1\" # Environment ID\n</code></pre> <p>You will need to login to your wandb account before you can run:</p> <pre><code>wandb login # only required for the first time\npython cleanrl/reinforce_quantum.py \\\n</code></pre> <p>This will create an additional folder for the wandb logging and you can inspect your experiment data also online:</p> <pre><code>...\nconfigs\nexamples\nlogs/\n    2025-03-04--14-59-32_reinforce_quantum_wandb    # The name of your experiment\n        wandb                                       # Subfolder of the wandb logging\n        config.yaml                                 # Config which was used to run this experiment\n        result.json                                 # Results of the experiment\n.gitignore\n...\n</code></pre>"},{"location":"get_started/#run-experiments-with-config-files","title":"Run experiments with config files","text":"<p>Additionally, all algorithms can be executed from an external script as well. There are two examples in the root directory <code>main.py</code> and <code>main_batch.py</code>. You can specify all parameters in a YAML file instead (and also reuse the <code>config.yaml</code>files which have been generated in previous runs). For examples, take a look at the <code>configs/basic</code>folder. You will just need to specify the config path. </p> main.py<pre><code>    config_path = \"configs/basic/reinforce_quantum.yaml\"\n\n    # Load the config file\n    with open(config_path) as f:\n        config = yaml.load(f, Loader=yaml.FullLoader)\n\n    # Based on the current time, create a unique name for the experiment\n    config[\"trial_name\"] = (\n        datetime.now().strftime(\"%Y-%m-%d--%H-%M-%S\") + \"_\" + config[\"trial_name\"]\n    )\n    config[\"path\"] = os.path.join(\n        os.path.dirname(os.getcwd()), config[\"trial_path\"], config[\"trial_name\"]\n    )\n\n    # Create the directory and save a copy of the config file so\n    # that the experiment can be replicated\n    os.makedirs(os.path.dirname(config[\"path\"] + \"/\"), exist_ok=True)\n    shutil.copy(config_path, os.path.join(config[\"path\"], \"config.yaml\"))\n\n    # Start the agent training\n    train_agent(config)\n</code></pre> <p>If you want to execute several config files sequentially, you can also you the <code>main_batch.py</code> file, where you can specify several configs in a list or execute all configs in a subdirectory.</p> main_batch.py<pre><code>    # Specify the path to the config file\n    # Get all config files in the configs folder\n    # config_files = [f for f in os.listdir('configs/basic') if f.endswith('.yaml')]\n    config_paths = [\n        \"configs/basic/dqn_classical.yaml\",\n        \"configs/basic/reinforce_classical.yaml\",\n        \"configs/basic/reinforce_classical_continuous_action.yaml\",\n        \"configs/basic/ppo_classical.yaml\",\n        \"configs/basic/ppo_classical_continuous_action.yaml\",\n    ]\n</code></pre>"},{"location":"get_started/#experiment-logging","title":"Experiment logging","text":"<p>By default, all metrics are logged to a <code>result.json</code> file on the experiment folder. Plots are generated by default for these runs as well for some of the metrics. For details, take a look at <code>cleanqrl_utils/plotting.py</code>. </p> <p>When using <code>wandb</code>, all data is additionally logged to your wandb account .</p>"},{"location":"advanced_usage/hyperparameter_tuning/","title":"Hyperparameter Tuning","text":""},{"location":"advanced_usage/hyperparameter_tuning/#ray-tune","title":"Ray Tune","text":"<p>Generally, in (Q)RL, hyperparameter tuning is essential. Therefore, we offer easy hyperparameter tunings with ray tune. To use it, you can take a look at the config files located in the <code>configs/tune</code> folder and the <code>tune.py</code>.</p> tune.py<pre><code>    # Generate the parameter space for the experiment from the config file\n    config = add_hyperparameters(config)\n\n    .... \n\n    # Instead of running a single agent as before, we will use ray.tune to run multiple agents\n    # in parallel. We will use the same train_agent function as before.\n    ray.init(\n        local_mode=config[\"ray_local_mode\"],\n        num_cpus=config[\"num_cpus\"],\n        num_gpus=config[\"num_gpus\"],\n        _temp_dir=os.path.join(os.path.dirname(os.getcwd()), \"t\"),\n        include_dashboard=False,\n    )\n\n    # We need an addtional function to create subfolders for each hyperparameter configuration\n    def trial_name_creator(trial):\n        return trial.__str__() + \"_\" + trial.experiment_tag\n\n    # We will use the tune.Tuner class to run multiple agents in parallel\n    trainable = tune.with_resources(\n        train_agent,\n        resources={\"cpu\": config[\"cpus_per_worker\"], \"gpu\": config[\"gpus_per_worker\"]},\n    )\n    tuner = tune.Tuner(\n        trainable,\n        param_space=config,\n        run_config=tune.RunConfig(storage_path=config[\"path\"]),\n        tune_config=tune.TuneConfig(\n            num_samples=config[\"num_samples\"],\n            trial_dirname_creator=trial_name_creator,\n        ),\n    )\n\n    # The fit function will start the hyperparameter search\n    tiral = tuner.fit()\n</code></pre> <p>We use the tune.Tuner to perform the hyperparameter search. But before we take a look at the hyperparameters, lets see first how to define the resources: In the config files, we have an additional block:</p> <p><pre><code># ray tune parameters\nray_local_mode:         False\nnum_cpus:               24\nnum_gpus:               0\nnum_samples:            3\ncpus_per_worker:        1\ngpus_per_worker:        0\n</code></pre> The first parameter defines if we want to use the so called <code>local_mode</code> which enforces sequential execution instead of parallel execution for debugging purposes. Hence, this should always be False if you want to run the actual training. Then you need to specify the amount of cpus and gpus you want to make available to the tune.Tuner. This depends on the machine you are using. Next, you need to define the number of samples you want to run for each hyperparameter configuration. Generally, you do not want the number to be too small, because especially QRL experiments can have large variances in performance. But also you don't want this number to be too big, because this will cause very long runtimes. Lastly, you need to define how many resources each worker, that is each sample, gets to run. E.g. If you specify <code>num_cpus=10</code> and <code>cpus_per_worker=1</code>, then 10 runs will be run in parallel.</p>"},{"location":"advanced_usage/hyperparameter_tuning/#add-hyperparameters","title":"Add hyperparameters","text":"<p>Adding hyperparameters to sample is straight forward. For that, you can look at the following example. Instead of specifying e.g. </p> <pre><code>lr_weights: 0.001 \n</code></pre> <p>You can instead specify it as:</p> <pre><code>lr: \n    - grid_search           # Do a grid search with number of seeds == num_samples\n    - float                 # Define the type [float, int, str, list]\n    - [0.01, 0.001, 0.0001] # List of parameters to select\n</code></pre> <p>If <code>num_samples</code> is set to 5 and <code>num_cpus</code> to 15, then each hyperparameter configuration will be run with 5 seeds in parallel (watch your RAM for larger qubit numbers).</p> <p>You can now also edit other variables:</p> <pre><code>batch_size: \n    - grid_search           # Do a grid search with number of seeds == num_samples\n    - int                   # Define the type [float, int, str, list]\n    - [16, 32]              # List of parameters to select\n</code></pre> <p>This will now start for each of the <code>batch_size</code> a hyperparameter run with all the specified learning rates for <code>num_samples</code>, so a total of 30 trials. If <code>num_cpus</code> is set to 10, then it will sequentially execute the 30 trials with 10 trials in parallel.</p> <p>You can even do a search over the environments as:</p> <pre><code>env_id: \n    - grid_search           # Do a grid search with number of seeds == num_samples\n    - str                   # Define the type [float, int, str, list]\n    - ['CartPole-v1',       # List of parameters to select\n       'Acrobot-v1']              \n</code></pre> <p>Finally, just like for the <code>main.py</code>, there exists also a <code>tune_batch.py</code> file, where you can sequentially perform hyperparameter runs.</p>"},{"location":"advanced_usage/jumanji_environments/","title":"Jumanji Environments","text":"<p>The Jumanji environments are integrated via the <code>create_jumanji_env</code> function, which initializes specific environments like TSP, Knapsack, and Maze from the jumanji.environments module. This function takes an <code>env_id</code> (e.g., \"TSP-v1\", \"Knapsack-v1\", \"Maze-v0\") and a config dictionary to customize parameters such as <code>num_cities</code>, <code>num_items</code>, or maze dimensions (<code>num_rows</code>, <code>num_cols</code>). It uses generators (e.g., <code>UniformGenerator</code> for TSP, <code>RandomGeneratorKnapsack</code> for Knapsack) to define problem instances. For unsupported <code>env_ids</code>, it falls back to jumanji.make with a warning about potential missing custom wrappers. </p> <p>The function applies standard wrappers like <code>JumanjiToGymWrapper</code>, <code>FlattenObservation</code>, and <code>RecordEpisodeStatistics</code> before adding custom wrappers, ensuring compatibility with Gymnasium\u2019s API. Note that the <code>FlattenObservation</code> gym wrapper converts the observation from a dictionary to a numpy array.</p>"},{"location":"advanced_usage/jumanji_environments/#jumanji-wrappers","title":"Jumanji wrappers","text":"<p>In this repository, there are three custom wrappers <code>JumanjiWrapperTSP</code>, <code>JumanjiWrapperKnapsack</code>, and <code>JumanjiWrapperMaze</code>. For additional Jumanji environments, you will need to write your own wrapper.</p> <ul> <li><code>JumanjiWrapperTSP</code>: Wraps the TSP environment, overriding <code>step</code> to track episode counts and compute additional metrics every 100 episodes when truncated. It extracts node coordinates from the state and calculates the optimal tour length using <code>tsp_compute_optimal_tour</code>, which exhaustively tests permutations via itertools.permutations. The <code>tsp_compute_tour_length</code> helper computes Euclidean distances between nodes, including the return to the start. The wrapper adds <code>optimal_tour_length</code> and <code>approximation_ratio</code> to the info dict and is penalizing incomplete tours. Note that the calculation of the <code>optimal_tour_lenght</code> is done in brute force, which will be very expensive for large problem sizes.</li> <li><code>JumanjiWrapperKnapsack</code>: Wraps the Knapsack environment, enhancing step to compute optimal values every 100 episodes upon truncation. It uses <code>knapsack_optimal_value</code>, a dynamic programming solution that discretizes weights and values (scaled by precision=1000) to solve the <code>0-1 knapsack problem</code> efficiently. The wrapper stores the previous state and adds <code>optimal_value</code> and <code>approximation_ratio</code> to info, enabling performance evaluation.</li> <li><code>JumanjiWrapperMaze</code>: Wraps the Maze environment, adding a configurable <code>reset</code> method that supports a constant maze layout via a fixed seed if <code>constant_maze</code> is <code>True</code> in the config. The <code>step</code> method simplifies <code>termination</code> handling, always returning <code>terminate=False</code> and clearing info unless truncated. This wrapper is minimal but allows for consistent maze testing.</li> </ul> <p>These wrappers are applied in <code>create_jumanji_env</code> based on the <code>env_id</code>, building on Jumanji\u2019s base environments and Gymnasium\u2019s utilities to provide a robust RL experimentation framework.</p>"},{"location":"algorithms/ddpg/","title":"DDPG","text":"<p>Deep Deterministic Policy Gradient (DDPG)</p>"},{"location":"algorithms/ddpg/#continuous-state-continuous-action","title":"Continuous state - continuous action","text":"<p>The <code>ddpg_classical.py</code> and the <code>ddpg_quantum.py</code> have the following features:</p> <ul> <li>\u2705 Work with the continuous observation space </li> <li>\u2705 Work with the continuous action space</li> <li>\u2705 Work with envs like Pendulum-v1</li> <li>\u2705 Multiple Vectorized Environments </li> <li>\u2705 Single file implementation </li> </ul>"},{"location":"algorithms/ddpg/#implementation-details","title":"Implementation details","text":"<p>The key difference between the classical and the quantum algorithm is the <code>ddpgAgentQuantum</code> class, as shown below</p> ddpg_quantum.py<pre><code>class ddpgAgentQuantum(nn.Module):\n    def __init__(self, observation_size, num_actions, config):\n        super().__init__()\n        self.config = config\n        self.observation_size = observation_size\n        self.num_actions = num_actions\n        self.num_qubits = config[\"num_qubits\"]\n        self.num_layers = config[\"num_layers\"]\n        # input and output scaling are always initialized as ones\n        self.input_scaling = nn.Parameter(\n            torch.ones(self.num_layers, self.num_qubits), requires_grad=True\n        )\n        self.output_scaling = nn.Parameter(\n            torch.ones(self.num_actions), requires_grad=True\n        )\n        # trainable weights are initialized randomly between -pi and pi\n        self.weights = nn.Parameter(\n            torch.FloatTensor(self.num_layers, self.num_qubits*2)\n            .uniform_(-np.pi, np.pi),\n            requires_grad=True,\n        )\n        device = qml.device(config[\"device\"], wires=range(self.num_qubits))\n        self.quantum_circuit = qml.QNode(\n            parameterized_quantum_circuit,\n            device,\n            diff_method=config[\"diff_method\"],\n            interface=\"torch\",\n        )\n</code></pre> ddpg_classical.py<pre><code>class ddpgAgentClassical(nn.Module):\n    def __init__(self, observation_size, num_actions):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Linear(observation_size, 64),\n            nn.ReLU(),\n            nn.Linear(64, 64),\n            nn.ReLU(),\n            nn.Linear(64, num_actions),\n        )\n</code></pre> ddpg_quantum.py<pre><code>def get_action_and_logprob(self, x):\n    logits = self.quantum_circuit(\n        x,\n        self.input_scaling,\n        self.weights,\n        self.num_qubits,\n        self.num_layers,\n        self.num_actions,\n        self.observation_size,\n    )\n    logits = torch.stack(logits, dim=1)\n    logits = logits * self.output_scaling\n    probs = Categorical(logits=logits)\n    action = probs.sample()\n    return action, probs.log_prob(action)\n</code></pre> ddpg_classical.py<pre><code>def get_action_and_logprob(self, x):\n    logits = self.network(x)\n    probs = Categorical(logits=logits)\n    action = probs.sample()\n    return action, probs.log_prob(action)\n</code></pre> <p>Additionally to these changes to the <code>Agent</code>class, we also need to specify a function for the ansatz of the parameterized quantum circuit. </p> ddpg_quantum.py<pre><code>def parameterized_quantum_circuit(\n    x, input_scaling, weights, num_qubits, num_layers, num_actions, observation_size\n):\n    for layer in range(num_layers):\n        for i in range(observation_size):\n            qml.RX(input_scaling[layer, i] * x[:, i], wires=[i])\n\n        for i in range(num_qubits):\n            qml.RY(weights[layer, i], wires=[i])\n\n        for i in range(num_qubits):\n            qml.RZ(weights[layer, i + num_qubits], wires=[i])\n\n        if num_qubits == 2:\n            qml.CZ(wires=[0, 1])\n        else:\n            for i in range(num_qubits):\n                qml.CZ(wires=[i, (i + 1) % num_qubits])\n\n    return [qml.expval(qml.PauliZ(wires=i)) for i in range(num_actions)]\n</code></pre> <p>In our implementation, the mean of the continuous action is based on the expectation value of the parameterized quantum circuit, while the variance is an additional classical trainable parameter. This parameter is also the same for all continuous actions. For additional information we refer to Variational Quantum Circuit Design for Quantum Reinforcement Learning on Continuous Environments.</p> <p>Our implementation implements some key novelties proposed by Skolik et al Quantum agents in the Gym.</p> <ul> <li><code>data reuploading</code>: In our ansatz, the features of the states are encoded via RX rotation gates. Instead of only encoding the features in the first layer, this process is repeated in each layer. This has shown to improve training performance.</li> <li><code>input scaling</code>: In our implementation, we define additionally to the trainable weights of the</li> <li><code>output scaling</code>: In our implementation, we define additionally to the trainable weights of the  </li> </ul> <p>We also provide the option to select different <code>learning rates</code> for the different parameter sets:</p> ddpg_quantum.py<pre><code>    optimizer = optim.Adam(\n        [\n            {\"params\": agent.input_scaling, \"lr\": lr_input_scaling},\n            {\"params\": agent.output_scaling, \"lr\": lr_output_scaling},\n            {\"params\": agent.weights, \"lr\": lr_weights},\n        ]\n    )\n</code></pre> <p>Also, you can use a faster pennylane backend for your simulations:</p> <ul> <li><code>pennylane-lightning</code>: We enable the use of the <code>lightning</code> simulation backend by pennylane, which speeds up simulation </li> </ul> <p>We also add an observation wrapper called <code>ArctanNormalizationWrapper</code> at the very beginning of the file. Because we encode the features of the states as rotations, we need to ensure that the features are not beyond the interval of - \u03c0 and \u03c0 due to the periodicity of the rotation gates. For more details on wrappers, see Advanced Usage.</p>"},{"location":"algorithms/ddpg/#experiment-results","title":"Experiment results","text":""},{"location":"algorithms/dqn/","title":"DQN","text":"<p>DQN builds upon Q-learning by introducing a replay buffer and target network, key innovations that enhance algorithm stability. For details, see the original paper Human-level control through deep reinforcement learning . </p>"},{"location":"algorithms/dqn/#continuous-state-discrete-action","title":"Continuous state - discrete action","text":"<p>The <code>dqn_classical.py</code> and the <code>dqn_quantum.py</code> have the following features:</p> <ul> <li>\u2705 Work with the Box observation space of low-level features</li> <li>\u2705 Work with the discrete action space</li> <li>\u2705 Work with envs like CartPole-v1</li> <li>\u274c Multiple Vectorized Environments not enabled</li> <li>\u274c No single file implementation (require <code>replay_buffer.py</code>)</li> </ul>"},{"location":"algorithms/dqn/#implementation-details","title":"Implementation details","text":"<p>Our implementation of the DQN is essentially the same as in CleanRL. For implementation details of the classical algorithm, we refer to the CleanRL documentation. The key difference between the classical and the quantum algorithm is the <code>DQNAgentQuantum</code> class, as shown below</p> dqn_quantum.py<pre><code>class DQNAgentQuantum(nn.Module):\n    def __init__(self, observation_size, num_actions, config):\n        super().__init__()\n        self.config = config\n        self.observation_size = observation_size\n        self.num_actions = num_actions\n        self.num_qubits = config[\"num_qubits\"]\n        self.num_layers = config[\"num_layers\"]\n        # input and output scaling are always initialized as ones\n        self.input_scaling = nn.Parameter(\n            torch.ones(self.num_layers, self.num_qubits), requires_grad=True\n        )\n        self.output_scaling = nn.Parameter(\n            torch.ones(self.num_actions), requires_grad=True\n        )\n        # trainable weights are initialized randomly between -pi and pi\n        self.weights = nn.Parameter(\n            torch.FloatTensor(self.num_layers, self.num_qubits*2)\n            .uniform_(-np.pi, np.pi),\n            requires_grad=True,\n        )\n        device = qml.device(config[\"device\"], wires=range(self.num_qubits))\n        self.quantum_circuit = qml.QNode(\n            parameterized_quantum_circuit,\n            device,\n            diff_method=config[\"diff_method\"],\n            interface=\"torch\",\n        )\n</code></pre> dqn_classical.py<pre><code>class DQNAgentClassical(nn.Module):\n    def __init__(self, observation_size, num_actions):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Linear(observation_size, 64),\n            nn.ReLU(),\n            nn.Linear(64, 64),\n            nn.ReLU(),\n            nn.Linear(64, num_actions),\n        )\n</code></pre> dqn_quantum.py<pre><code>def forward(self, x):\n    logits = self.quantum_circuit(\n        x,\n        self.input_scaling,\n        self.weights,\n        self.num_qubits,\n        self.num_layers,\n        self.num_actions,\n        self.observation_size\n    )\n    logits = torch.stack(logits, dim=1)\n    logits = logits * self.output_scaling\n    return logits\n</code></pre> dqn_classical.py<pre><code>def forward(self, x):\n    return self.network(x)\n</code></pre> <p>Additionally, to these changes to the <code>Agent</code>class, we also need to specify a function for the ansatz of the parameterized quantum circuit. </p> dqn_quantum.py<pre><code>def parameterized_quantum_circuit(\n    x, input_scaling, weights, num_qubits, num_layers, num_actions, observation_size\n):\n    for layer in range(num_layers):\n        for i in range(observation_size):\n            qml.RX(input_scaling[layer, i] * x[:, i], wires=[i])\n\n        for i in range(num_qubits):\n            qml.RY(weights[layer, i], wires=[i])\n\n        for i in range(num_qubits):\n            qml.RZ(weights[layer, i + num_qubits], wires=[i])\n\n        if num_qubits == 2:\n            qml.CZ(wires=[0, 1])\n        else:\n            for i in range(num_qubits):\n                qml.CZ(wires=[i, (i + 1) % num_qubits])\n\n    return [qml.expval(qml.PauliZ(wires=i)) for i in range(num_actions)]\n</code></pre> <p>The ansatz of this parameterized quantum circuit is taken from the publication by Skolik et al Quantum agents in the Gym. The ansatz is also depicted in the figure below:</p> <p>Our implementation hence incorporates some key novelties proposed by Skolik:</p> <ul> <li><code>data reuploading</code>: In our ansatz, the features of the states are encoded via RX rotation gates. Instead of only encoding the features in the first layer, this process is repeated in each layer. This has shown to improve training performance.</li> <li><code>input scaling</code>: In our implementation, we define additionally to the trainable weights of the</li> <li><code>output scaling</code>: In our implementation, we define additionally to the trainable weights of the  </li> </ul> <p>We also provide the option to select different <code>learning rates</code> for the different parameter sets:</p> dqn_quantum.py<pre><code>    optimizer = optim.Adam(\n        [\n            {\"params\": agent.input_scaling, \"lr\": lr_input_scaling},\n            {\"params\": agent.output_scaling, \"lr\": lr_output_scaling},\n            {\"params\": agent.weights, \"lr\": lr_weights},\n        ]\n    )\n</code></pre> <p>Also, you can use a faster pennylane backend for your simulations:</p> <ul> <li><code>pennylane-lightning</code>: We enable the use of the <code>lightning</code> simulation backend by pennylane, which speeds up simulation </li> </ul> <p>We also add an observation wrapper called <code>ArctanNormalizationWrapper</code> at the very beginning of the file. Because we encode the features of the states as rotations, we need to ensure that the features are not beyond the interval of - \u03c0 and \u03c0 due to the periodicity of the rotation gates. For more details on wrappers, see Advanced Usage.</p>"},{"location":"algorithms/dqn/#experiment-results","title":"Experiment results","text":"<p>Next to the </p>"},{"location":"algorithms/dqn/#discrete-state-discrete-action","title":"Discrete state - discrete action","text":"<p>The <code>dqn_classical_discrete_state.py</code> and the <code>dqn_quantum_discrete_state.py</code> have the following features:</p> <ul> <li>\u2705 Work with the discrete observation space </li> <li>\u2705 Work with the discrete action space</li> <li>\u2705 Work with envs like FrozenLake-v1</li> <li>\u274c Multiple Vectorized Environments not enabled</li> <li>\u274c No single file implementation (require <code>replay_buffer.py</code>)</li> </ul>"},{"location":"algorithms/dqn/#implementation-details_1","title":"Implementation details","text":"<p>The implementations follow the same principles as <code>dqn_classical.py</code> and <code>dqn_quantum.py</code>. In the following we focus on the key differences between <code>dqn_quantum_discrete_state.py</code> and <code>dqn_quantum.py</code>. The same differences also apply for the classical implementations.</p> <p>The key difference is the state encoding. Since discrete state environments like FrozenLake return an integer value, it is straight forward to encode the state as a binary value instead of an integer. For that, an additional function for <code>DQNAgentQuantuM</code> is added called <code>encoding_input</code>. This converts the integer value into its binary value.</p> <p>dqn_quantum_discrete_state.py<pre><code>    def encode_input(self, x):\n        x_binary = torch.zeros((x.shape[0], self.observation_size))\n        for i, val in enumerate(x):\n            binary = bin(int(val.item()))[2:]\n            padded = binary.zfill(self.observation_size)\n            x_binary[i] = torch.tensor([int(bit) * np.pi for bit in padded])\n        return x_binary\n</code></pre> Now we just need to also call this function before we pass the input to the parameterized quantum circuit:</p> dqn_quantum_discrete_state.py<pre><code>    def forward(self, x):\n        x_encoded = self.encode_input(x)\n        logits = self.quantum_circuit(\n            x_encoded,\n            self.input_scaling,\n            self.weights,\n            self.num_qubits,\n            self.num_layers,\n            self.num_actions,\n            self.observation_size\n        )\n        logits = torch.stack(logits, dim=1)\n        logits = logits * self.output_scaling\n        return logits\n</code></pre>"},{"location":"algorithms/dqn/#experiment-results_1","title":"Experiment results","text":""},{"location":"algorithms/dqn/#jumanji-environments","title":"Jumanji Environments","text":"<p>The <code>dqn_classical_jumanji.py</code> and the <code>dqn_quantum_jumanji.py</code> have the following features:</p> <ul> <li>\u2705 Work with jumanji environments </li> <li>\u2705 Work with envs like Traveling Salesperson and Knapsack</li> <li>\u274c Multiple Vectorized Environments not enabled</li> <li>\u274c No single file implementation (require <code>replay_buffer.py</code>)</li> <li>\u274c Require custom wrapper file for jumanji (<code>jumanji_wrapper.py</code>)</li> </ul>"},{"location":"algorithms/dqn/#implementation-details_2","title":"Implementation details","text":"<p>The implementations follow the same principles as <code>dqn_classical.py</code> and <code>dqn_quantum.py</code>. In the following we focus on the key differences between <code>dqn_quantum_jumanji.py</code> and <code>dqn_quantum.py</code>. The same differences also apply for the classical implementations.</p> <p>For most of the <code>`jumanji</code> environments, the observation space is quite complex. Instead of simple numpy arrays for the states, we often have dictionary states which vary in size and shape. E.g. the Knapsack problem returns a state of shape </p> <ul> <li><code>weights</code>: jax array (float) of shape (num_items,), array of weights of the items to be packed into the knapsack.</li> <li><code>values</code>: jax array (float) of shape (num_items,), array of values of the items to be packed into the knapsack.</li> <li><code>packed_items</code>: jax array (bool) of shape (num_items,), array of binary values denoting which items are already packed into the knapsack.</li> <li><code>action_mask</code>: jax array (bool) of shape (num_items,), array of binary values denoting which items can be packed into the knapsack.</li> </ul> <p>In order to parse this to a parameterized quantum circuit or a neural network, we can use a gym wrapper which converters the state again to an array. This is being done when calling the function <code>create_jumanji_wrapper</code>. For more details see Jumanji Wrapper. </p> dqn_quantum_jumanji.py<pre><code>def make_env(env_id, config):\n    def thunk():\n        env = create_jumanji_env(env_id, config)\n        env = ReplayBufferWrapper(env)\n\n        return env\n\n    return thunk\n</code></pre> dqn_quantum_jumanji.py<pre><code>class DQNAgentQuantum(nn.Module):\n    def __init__(self, num_actions, config):\n        super().__init__()\n        self.config = config\n        self.num_actions = num_actions\n        self.num_qubits = config[\"num_qubits\"]\n        self.num_layers = config[\"num_layers\"]\n        self.block_size = 3  # number of subblocks depends on environment\n</code></pre> <p>Because the state is now growing quickly in size as the number of items of the Knapsack is increased, we use a slightly different approach to encode it: Instead of encoding each feature of the state on an individual qubit, we divide the state again into 3 blocks, namely <code>weights</code>, <code>values</code>and <code>packed_items</code>. This will be important for the modification of our ansatz for the parameterized quantum circuit which you can see below:</p> dqn_quantum_jumanji.py<pre><code>def parametrized_quantum_circuit(\n    x, input_scaling, weights, num_qubits, num_layers, num_actions\n):\n\n    # This block needs to be adapted depending on the environment.\n    # The input vector is of shape [4*num_actions] for the Knapsack:\n    # [action mask, packed items, values, weights]\n\n    annotations = x[:, num_qubits : num_qubits * 2]\n    values_kp = x[:, num_qubits * 2 : num_qubits * 3]\n    weights_kp = x[:, 3*num_qubits:]\n\n    for layer in range(num_layers):\n        for block, features in enumerate([annotations, values_kp, weights_kp]):\n            for i in range(num_qubits):\n                qml.RX(input_scaling[layer, block, i] * features[:, i], wires=[i])\n\n            for i in range(num_qubits):\n                qml.RY(weights[layer, block, i], wires=[i])\n\n            for i in range(num_qubits):\n                qml.RZ(weights[layer, block, i+num_qubits], wires=[i])\n\n            if num_qubits == 2:\n                qml.CZ(wires=[0, 1])\n            else:\n                for i in range(num_qubits):\n                    qml.CZ(wires=[i, (i + 1) % num_qubits])\n\n    return [qml.expval(qml.PauliZ(wires=i)) for i in range(num_actions)]\n</code></pre> <p>We encode each of these blocks individually in each layer. By that, we can save quantum circuit width, so the number of qubits, by increasing our quantum circuit depth, so the number of gates we are using. See our Tutorials section for better ansatz design.</p>"},{"location":"algorithms/dqn/#experiment-results_2","title":"Experiment results","text":""},{"location":"algorithms/overview/","title":"Algorithms","text":""},{"location":"algorithms/overview/#overview","title":"Overview","text":"Algorithm Continuous state - discrete action Continuous state - continuous action Discrete state - discrete action Jumanji DQN <code>dqn_classical.py</code>, docs - <code>dqn_classical_discrete_state.py</code>, docs <code>dqn_classical_jumanji.py</code>, docs <code>dqn_quantum.py</code>, docs - <code>dqn_quantum_discrete_state.py</code>, docs <code>dqn_quantum_jumanji.py</code>, docs REINFORCE <code>reinforce_classical.py</code>, docs <code>reinforce_classical_continuous_action.py</code>, docs <code>reinforce_classical_discrete_state.py</code>, docs <code>reinforce_classical_jumanji.py</code>, docs <code>reinforce_quantum.py</code>, docs <code>reinforce_quantum_continuous_action.py</code>, docs <code>reinforce_quantum_discrete_state.py</code>, docs <code>reinforce_quantum_jumanji.py</code>, docs PPO <code>ppo_classical.py</code>, docs <code>ppo_classical_continuous_action.py</code>, docs <code>ppo_classical_discrete_state.py</code>, docs <code>ppo_classical_jumanji.py</code>, docs <code>ppo_quantum.py</code>, docs <code>ppo_quantum_continuous_action.py</code>, docs <code>ppo_quantum_discrete_state.py</code>, docs <code>ppo_quantum_jumanji.py</code>, docs DDPG - <code>ddpg_classical_continuous_action.py</code>, docs - - - <code>ddpg_quantum_continuous_action.py</code>, docs - -"},{"location":"algorithms/ppo/","title":"PPO","text":"<p>https://arxiv.org/pdf/1509.02971.pdf) Proximal Policy Optimization (PPO) has become a leading algorithm in deep reinforcement learning due to its robust performance and relative simplicity. It can utilize parallel environments for faster training and supports diverse action spaces, enabling its application to a wide variety of tasks, including many games. PPO achieves stability by limiting the policy update at each step, preventing drastic changes that can derail learning \u2013 a key advantage over earlier policy gradient methods. Furthermore, it exhibits better sample efficiency than algorithms like DQN. Our implementation mainly follows the one provided by CleanRL.</p>"},{"location":"algorithms/ppo/#continuous-state-discrete-action","title":"Continuous state - discrete action","text":"<p>The <code>ppo_classical.py</code> and the <code>ppo_quantum.py</code> have the following features:</p> <ul> <li>\u2705 Work with the Box observation space of low-level features</li> <li>\u2705 Work with the discrete action space</li> <li>\u2705 Work with envs like CartPole-v1</li> <li>\u2705 Multiple Vectorized Environments </li> <li>\u2705 Single file implementation </li> </ul>"},{"location":"algorithms/ppo/#implementation-details","title":"Implementation details","text":"<p>The key difference between the classical and the quantum algorithm is the <code>PPOAgentQuantum</code> class, as shown below</p> ppo_quantum.py<pre><code>class PPOAgentQuantum(nn.Module):\n    def __init__(self, observation_size, num_actions, config):\n        super().__init__()\n        self.config = config\n        self.observation_size = observation_size\n        self.num_actions = num_actions\n        self.num_qubits = config[\"num_qubits\"]\n        self.num_layers = config[\"num_layers\"]\n\n        # input and output scaling are always initialized as ones\n        self.input_scaling_critic = nn.Parameter(\n            torch.ones(self.num_layers, self.num_qubits), requires_grad=True\n        )\n        self.output_scaling_critic = nn.Parameter(torch.tensor(1.0), requires_grad=True)\n        # trainable weights are initialized randomly between -pi and pi\n        self.weights_critic = nn.Parameter(\n            torch.FloatTensor(self.num_layers, self.num_qubits * 2).uniform_(\n                -np.pi, np.pi\n            ),\n            requires_grad=True,\n        )\n\n        # input and output scaling are always initialized as ones\n        self.input_scaling_actor = nn.Parameter(\n            torch.ones(self.num_layers, self.num_qubits), requires_grad=True\n        )\n        self.output_scaling_actor = nn.Parameter(\n            torch.ones(self.num_actions), requires_grad=True\n        )\n        # trainable weights are initialized randomly between -pi and pi\n        self.weights_actor = nn.Parameter(\n            torch.FloatTensor(self.num_layers, self.num_qubits * 2).uniform_(\n                -np.pi, np.pi\n            ),\n            requires_grad=True,\n        )\n\n        device = qml.device(config[\"device\"], wires=range(self.num_qubits))\n        self.quantum_circuit = qml.QNode(\n            parameterized_quantum_circuit,\n            device,\n            diff_method=config[\"diff_method\"],\n            interface=\"torch\",\n        )\n</code></pre> ppo_classical.py<pre><code>class PPOAgentClassical(nn.Module):\n    def __init__(self, envs):\n        super().__init__()\n        self.critic = nn.Sequential(\n            nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64),\n            nn.ReLU(),\n            nn.Linear(64, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1),\n        )\n        self.actor = nn.Sequential(\n            nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64),\n            nn.ReLU(),\n            nn.Linear(64, 64),\n            nn.ReLU(),\n            nn.Linear(64, envs.single_action_space.n),\n        )\n</code></pre> ppo_quantum.py<pre><code>def get_value(self, x):\n    value = self.quantum_circuit(\n        x,\n        self.input_scaling_critic,\n        self.weights_critic,\n        self.num_qubits,\n        self.num_layers,\n        self.num_actions,\n        self.observation_size,\n        \"critic\",\n    )\n    value = torch.stack(value, dim=1)\n    value = value * self.output_scaling_critic\n    return value\n\ndef get_action_and_value(self, x, action=None):\n    logits = self.quantum_circuit(\n        x,\n        self.input_scaling_actor,\n        self.weights_actor,\n        self.num_qubits,\n        self.num_layers,\n        self.num_actions,\n        self.observation_size,\n        \"actor\",\n    )\n    logits = torch.stack(logits, dim=1)\n    logits = logits * self.output_scaling_actor\n    probs = Categorical(logits=logits)\n    if action is None:\n        action = probs.sample()\n    return action, probs.log_prob(action), probs.entropy(), self.get_value(x)\n</code></pre> ppo_classical.py<pre><code>def get_value(self, x):\n    return self.critic(x)\n\ndef get_action_and_value(self, x, action=None):\n    logits = self.actor(x)\n    probs = Categorical(logits=logits)\n    if action is None:\n        action = probs.sample()\n    return action, probs.log_prob(action), probs.entropy(), self.critic(x)\n</code></pre> <p>Additionally to these changes to the <code>Agent</code>class, we also need to specify a function for the ansatz of the parameterized quantum circuit. We can reuse most of the circuit by passing an additional parameter which we will call <code>agent_type</code>. By doing so, we ensure that we either return a single expectation value for the critic or a tensor of expectation values of the shape <code>num_actions</code>:</p> ppo_quantum.py<pre><code>def parameterized_quantum_circuit(\n    x, input_scaling, weights, num_qubits, num_layers, num_actions, observation_size, agent_type\n):\n    for layer in range(num_layers):\n        for i in range(observation_size):\n            qml.RX(input_scaling[layer, i] * x[:, i], wires=[i])\n\n        for i in range(num_qubits):\n            qml.RY(weights[layer, i], wires=[i])\n\n        for i in range(num_qubits):\n            qml.RZ(weights[layer, i + num_qubits], wires=[i])\n\n        if num_qubits == 2:\n            qml.CZ(wires=[0, 1])\n        else:\n            for i in range(num_qubits):\n                qml.CZ(wires=[i, (i + 1) % num_qubits])\n\n    if agent_type == \"actor\":\n        return [qml.expval(qml.PauliZ(wires=i)) for i in range(num_actions)]\n    elif agent_type == \"critic\":\n        return [qml.expval(qml.PauliZ(0))]\n</code></pre> <p>The ansatz of this parameterized quantum circuit is taken from the publication of by Skolik et al Quantum agents in the Gym. The ansatz is also depicted in the figure below:</p> <p>Our implementation hence build open the work by Jerbi but also incorporates some key novelties proposed by Skolik:</p> <ul> <li><code>data reuploading</code>: In our ansatz, the features of the states are encoded via RX rotation gates. Instead of only encoding the features in the first layer, this process is repeated in each layer. This has shown to improve training performance.</li> <li><code>input scaling</code>: In our implementation, we define additionally to the trainable weights of the</li> <li><code>output scaling</code>: In our implementation, we define additionally to the trainable weights of the  </li> </ul> <p>We also provide the option to select different <code>learning rates</code> for the different parameter sets for the actor and the critic:</p> ppo_quantum.py<pre><code>    optimizer = optim.Adam(\n        [\n            {\"params\": agent.input_scaling_actor, \"lr\": lr_input_scaling},\n            {\"params\": agent.output_scaling_actor, \"lr\": lr_output_scaling},\n            {\"params\": agent.weights_actor, \"lr\": lr_weights},\n            {\"params\": agent.input_scaling_critic, \"lr\": lr_input_scaling},\n            {\"params\": agent.output_scaling_critic, \"lr\": lr_output_scaling},\n            {\"params\": agent.weights_critic, \"lr\": lr_weights},\n        ]\n    )\n</code></pre> <p>Also, you can use a faster pennylane backend for your simulations:</p> <ul> <li><code>pennylane-lightning</code>: We enable the use of the <code>lightning</code> simulation backend by pennylane, which speeds up simulation </li> </ul> <p>We also add an observation wrapper called <code>ArctanNormalizationWrapper</code> at the very beginning of the file. Because we encode the features of the states as rotations, we need to ensure that the features are not beyond the interval of - \u03c0 and \u03c0 due to the periodicity of the rotation gates. For more details on wrappers, see Advanced Usage.</p>"},{"location":"algorithms/ppo/#experiment-results","title":"Experiment results","text":""},{"location":"algorithms/ppo/#continuous-state-continuous-action","title":"Continuous state - continuous action","text":"<p>The <code>ppo_classical_continuous_action.py</code> and the <code>ppo_quantum_continuous_action.py</code> have the following features:</p> <ul> <li>\u2705 Work with the continuous observation space </li> <li>\u2705 Work with the continuous action space</li> <li>\u2705 Work with envs like Pendulum-v1</li> <li>\u2705 Multiple Vectorized Environments </li> <li>\u2705 Single file implementation </li> </ul>"},{"location":"algorithms/ppo/#implementation-details_1","title":"Implementation details","text":"<p>The implementations follow the same principles as <code>ppo_classical.py</code> and <code>ppo_quantum.py</code>. In the following we focus on the key differences between <code>ppo_quantum_continuous_action.py</code> and <code>ppo_quantum.py</code>. The same differences also apply for the classical implementations.</p> <p>While the state encoding is the same as for the previous approach, we need to implement some modifications in order to draw continuous actions with the parameterized quantum circuit. For that we modify the <code>PPOAgentQuantum</code> class as follows:</p> ppo_quantum_continuous_action.py<pre><code>class PPOAgentQuantumContinuous(nn.Module):\n    def __init__(self, observation_size, num_actions, config):\n        super().__init__()\n        self.config = config\n        self.observation_size = observation_size\n        self.num_actions = num_actions\n        self.num_qubits = config[\"num_qubits\"]\n        self.num_layers = config[\"num_layers\"]\n\n        .....\n\n\n        # additional trainable parameters for the variance of the continuous actions\n        self.actor_logstd = nn.Parameter(torch.zeros(1, num_actions))\n\n    def get_action_and_value(self, x, action=None):\n        action_mean = self.quantum_circuit(\n            x,\n            self.input_scaling_actor,\n            self.weights_actor,\n            self.num_qubits,\n            self.num_layers,\n            self.num_actions,\n            self.observation_size,\n            \"actor\",\n        )\n        action_mean = torch.stack(action_mean, dim=1)\n        action_mean = action_mean * self.output_scaling_actor\n        action_logstd = self.actor_logstd.expand_as(action_mean)\n        action_std = torch.exp(action_logstd)\n        probs = Normal(action_mean, action_std)\n        if action is None:\n            action = probs.sample()\n        return (\n            action,\n            probs.log_prob(action).sum(1),\n            probs.entropy().sum(1),\n            self.get_value(x),\n        )\n</code></pre> <p>In our implementation, the mean of the continuous action is based on the expectation value of the parameterized quantum circuit, while the variance is an additional classical trainable parameter. This parameter is also the same for all continuous actions. For additional information we refer to Variational Quantum Circuit Design for Quantum Reinforcement Learning on Continuous Environments.</p>"},{"location":"algorithms/ppo/#experiment-results_1","title":"Experiment results","text":""},{"location":"algorithms/ppo/#discrete-state-discrete-action","title":"Discrete state - discrete action","text":"<p>The <code>ppo_classical_discrete_state.py</code> and the <code>ppo_quantum_discrete_state.py</code> have the following features:</p> <ul> <li>\u2705 Work with the discrete observation space </li> <li>\u2705 Work with the discrete action space</li> <li>\u2705 Work with envs like FrozenLake-v1</li> <li>\u2705 Multiple Vectorized Environments </li> <li>\u2705 Single file implementation </li> </ul>"},{"location":"algorithms/ppo/#implementation-details_2","title":"Implementation details","text":"<p>The implementations follow the same principles as <code>ppo_classical.py</code> and <code>ppo_quantum.py</code>. In the following we focus on the key differences between <code>ppo_quantum_discrete_state.py</code> and <code>ppo_quantum.py</code>. The same differences also apply for the classical implementations.</p> <p>The key difference is the state encoding. Since discrete state environments like FrozenLake return an integer value, it is straight forward to encode the state as a binary value instead of an integer. For that, an additional function for <code>PPOAgentQuantum</code> is added called <code>encoding_input</code>. This converts the integer value into its binary value.</p> <p>ppo_quantum_discrete_state.py<pre><code>    def encode_input(self, x):\n        x_binary = torch.zeros((x.shape[0], self.observation_size))\n        for i, val in enumerate(x):\n            binary = bin(int(val.item()))[2:]\n            padded = binary.zfill(self.observation_size)\n            x_binary[i] = torch.tensor([int(bit) * np.pi for bit in padded])\n        return x_binary\n</code></pre> Now we just need to also call this function before we pass the input to the parameterized quantum circuit:</p> ppo_quantum_discrete_state.py<pre><code>    def get_action_and_value(self, x, action=None):\n        x_encoded = self.encode_input(x)\n        logits = self.quantum_circuit(\n            x_encoded,\n            self.input_scaling_actor,\n            self.weights_actor,\n            self.num_qubits,\n            self.num_layers,\n            self.num_actions,\n            self.observation_size,\n            \"actor\",\n        )\n        logits = torch.stack(logits, dim=1)\n        logits = logits * self.output_scaling_actor\n        probs = Categorical(logits=logits)\n        if action is None:\n            action = probs.sample()\n        return action, probs.log_prob(action), probs.entropy(), self.get_value(x)\n</code></pre>"},{"location":"algorithms/ppo/#experiment-results_2","title":"Experiment results","text":""},{"location":"algorithms/ppo/#jumanji-environments","title":"Jumanji Environments","text":"<p>The <code>ppo_classical_jumanji.py</code> and the <code>ppo_quantum_jumanji.py</code> have the following features:</p> <ul> <li>\u2705 Work with jumanji environments </li> <li>\u2705 Work with envs like Traveling Salesperson and Knapsack</li> <li>\u2705 Multiple Vectorized Environments </li> <li>\u274c No single file implementation (require custom wrapper file for jumanji <code>jumanji_wrapper.py</code>)</li> </ul>"},{"location":"algorithms/ppo/#implementation-details_3","title":"Implementation details","text":"<p>The implementations follow the same principles as <code>ppo_classical.py</code> and <code>ppo_quantum.py</code>. In the following we focus on the key differences between <code>ppo_quantum_jumanji.py</code> and <code>ppo_quantum.py</code>. The same differences also apply for the classical implementations.</p> <p>For most of the <code>`jumanji</code> environments, the observation space is quite complex. Instead of simple numpy arrays for the states, we often have dictionary states which vary in size and shape. E.g. the Knapsack problem returns a state of shape </p> <ul> <li><code>weights</code>: jax array (float) of shape (num_items,), array of weights of the items to be packed into the knapsack.</li> <li><code>values</code>: jax array (float) of shape (num_items,), array of values of the items to be packed into the knapsack.</li> <li><code>packed_items</code>: jax array (bool) of shape (num_items,), array of binary values denoting which items are already packed into the knapsack.</li> <li><code>action_mask</code>: jax array (bool) of shape (num_items,), array of binary values denoting which items can be packed into the knapsack.</li> </ul> <p>In order to parse this to a parameterized quantum circuit or a neural network, we can use a gym wrapper which converters the state again to an array. This is being done when calling the function <code>create_jumanji_wrapper</code>. For more details see Jumanji Wrapper. </p> ppo_quantum_jumanji.py<pre><code>def make_env(env_id, config):\n    def thunk():\n        env = create_jumanji_env(env_id, config)\n\n        return env\n\n    return thunk\n</code></pre> ppo_quantum_jumanji.py<pre><code>class PPOAgentQuantum(nn.Module):\n    def __init__(self, num_actions, config):\n        super().__init__()\n        self.config = config\n        self.num_actions = num_actions\n        self.num_qubits = config[\"num_qubits\"]\n        self.num_layers = config[\"num_layers\"]\n        self.block_size = 3  # number of subblocks depends on environment\n</code></pre> <p>Because the state is now growing quickly in size as the number of items of the Knapsack is increased, we use a slightly different approach to encode it: Instead of encoding each feature of the state on an individual qubit, we divide the state again into 3 blocks, namely <code>weights</code>, <code>values</code>and <code>packed_items</code>. This will be important for the modification of our ansatz for the parameterized quantum circuit which you can see below:</p> ppo_quantum_jumanji.py<pre><code>def parametrized_quantum_circuit(\n    x, input_scaling, weights, num_qubits, num_layers, num_actions, agent_type\n):\n\n    # This block needs to be adapted depending on the environment.\n    # The input vector is of shape [4*num_actions] for the Knapsack:\n    # [action mask, packed items, values, weights]\n\n    annotations = x[:, num_qubits : num_qubits * 2]\n    values_kp = x[:, num_qubits * 2 : num_qubits * 3]\n    weights_kp = x[:, 3*num_qubits:]\n\n    for layer in range(num_layers):\n        for block, features in enumerate([annotations, values_kp, weights_kp]):\n            for i in range(num_qubits):\n                qml.RX(input_scaling[layer, block, i] * features[:, i], wires=[i])\n\n            for i in range(num_qubits):\n                qml.RY(weights[layer, block, i], wires=[i])\n\n            for i in range(num_qubits):\n                qml.RZ(weights[layer, block, i+num_qubits], wires=[i])\n\n            if num_qubits == 2:\n                qml.CZ(wires=[0, 1])\n            else:\n                for i in range(num_qubits):\n                    qml.CZ(wires=[i, (i + 1) % num_qubits])\n\n    if agent_type == \"actor\":\n        return [qml.expval(qml.PauliZ(wires=i)) for i in range(num_actions)]\n    elif agent_type == \"critic\":\n        return [qml.expval(qml.PauliZ(0))]\n</code></pre> <p>We encode each of these blocks individually in each layer. By that, we can save quantum circuit width, so the number of qubits, by increasing our quantum circuit depth, so the number of gates we are using. However, this still is not an optimal encoding. See our Tutorials section for better ansatz design.</p>"},{"location":"algorithms/ppo/#experiment-results_3","title":"Experiment results","text":""},{"location":"algorithms/reinforce/","title":"REINFORCE","text":"<p>REINFORCE is a Monte Carlo policy gradient algorithm that directly optimizes the policy parameters by estimating the gradient of the expected cumulative reward. This estimation is achieved through trajectory sampling and the application of the policy gradient theorem. For details, see the original paper Policy Gradient Methods for Reinforcement Learning with Function Approximation. In our implementation, we follow the steps of the paper by Jerbi Parametrized Quantum Policies for Reinforcement Learning.</p>"},{"location":"algorithms/reinforce/#continuous-state-discrete-action","title":"Continuous state - discrete action","text":"<p>The <code>reinforce_classical.py</code> and the <code>reinforce_quantum.py</code> have the following features:</p> <ul> <li>\u2705 Work with the Box observation space of low-level features</li> <li>\u2705 Work with the discrete action space</li> <li>\u2705 Work with envs like CartPole-v1</li> <li>\u2705 Multiple Vectorized Environments </li> <li>\u2705 Single file implementation </li> </ul>"},{"location":"algorithms/reinforce/#implementation-details","title":"Implementation details","text":"<p>The key difference between the classical and the quantum algorithm is the <code>ReinforceAgentQuantum</code> class, as shown below</p> reinforce_quantum.py<pre><code>class ReinforceAgentQuantum(nn.Module):\n    def __init__(self, observation_size, num_actions, config):\n        super().__init__()\n        self.config = config\n        self.observation_size = observation_size\n        self.num_actions = num_actions\n        self.num_qubits = config[\"num_qubits\"]\n        self.num_layers = config[\"num_layers\"]\n        # input and output scaling are always initialized as ones\n        self.input_scaling = nn.Parameter(\n            torch.ones(self.num_layers, self.num_qubits), requires_grad=True\n        )\n        self.output_scaling = nn.Parameter(\n            torch.ones(self.num_actions), requires_grad=True\n        )\n        # trainable weights are initialized randomly between -pi and pi\n        self.weights = nn.Parameter(\n            torch.FloatTensor(self.num_layers, self.num_qubits*2)\n            .uniform_(-np.pi, np.pi),\n            requires_grad=True,\n        )\n        device = qml.device(config[\"device\"], wires=range(self.num_qubits))\n        self.quantum_circuit = qml.QNode(\n            parameterized_quantum_circuit,\n            device,\n            diff_method=config[\"diff_method\"],\n            interface=\"torch\",\n        )\n</code></pre> reinforce_classical.py<pre><code>class ReinforceAgentClassical(nn.Module):\n    def __init__(self, observation_size, num_actions):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Linear(observation_size, 64),\n            nn.ReLU(),\n            nn.Linear(64, 64),\n            nn.ReLU(),\n            nn.Linear(64, num_actions),\n        )\n</code></pre> reinforce_quantum.py<pre><code>def get_action_and_logprob(self, x):\n    logits = self.quantum_circuit(\n        x,\n        self.input_scaling,\n        self.weights,\n        self.num_qubits,\n        self.num_layers,\n        self.num_actions,\n        self.observation_size,\n    )\n    logits = torch.stack(logits, dim=1)\n    logits = logits * self.output_scaling\n    probs = Categorical(logits=logits)\n    action = probs.sample()\n    return action, probs.log_prob(action)\n</code></pre> reinforce_classical.py<pre><code>def get_action_and_logprob(self, x):\n    logits = self.network(x)\n    probs = Categorical(logits=logits)\n    action = probs.sample()\n    return action, probs.log_prob(action)\n</code></pre> <p>Additionally to these changes to the <code>Agent</code>class, we also need to specify a function for the ansatz of the parameterized quantum circuit. </p> reinforce_quantum.py<pre><code>def parameterized_quantum_circuit(\n    x, input_scaling, weights, num_qubits, num_layers, num_actions, observation_size\n):\n    for layer in range(num_layers):\n        for i in range(observation_size):\n            qml.RX(input_scaling[layer, i] * x[:, i], wires=[i])\n\n        for i in range(num_qubits):\n            qml.RY(weights[layer, i], wires=[i])\n\n        for i in range(num_qubits):\n            qml.RZ(weights[layer, i + num_qubits], wires=[i])\n\n        if num_qubits == 2:\n            qml.CZ(wires=[0, 1])\n        else:\n            for i in range(num_qubits):\n                qml.CZ(wires=[i, (i + 1) % num_qubits])\n\n    return [qml.expval(qml.PauliZ(wires=i)) for i in range(num_actions)]\n</code></pre> <p>The ansatz of this parameterized quantum circuit is taken from the publication of by Skolik et al Quantum agents in the Gym. The ansatz is also depicted in the figure below:</p> <p>Our implementation hence build open the work by Jerbi but also incorporates some key novelties proposed by Skolik:</p> <ul> <li><code>data reuploading</code>: In our ansatz, the features of the states are encoded via RX rotation gates. Instead of only encoding the features in the first layer, this process is repeated in each layer. This has shown to improve training performance.</li> <li><code>input scaling</code>: In our implementation, we define additionally to the trainable weights of the</li> <li><code>output scaling</code>: In our implementation, we define additionally to the trainable weights of the  </li> </ul> <p>We also provide the option to select different <code>learning rates</code> for the different parameter sets:</p> reinforce_quantum.py<pre><code>    optimizer = optim.Adam(\n        [\n            {\"params\": agent.input_scaling, \"lr\": lr_input_scaling},\n            {\"params\": agent.output_scaling, \"lr\": lr_output_scaling},\n            {\"params\": agent.weights, \"lr\": lr_weights},\n        ]\n    )\n</code></pre> <p>Also, you can use a faster pennylane backend for your simulations:</p> <ul> <li><code>pennylane-lightning</code>: We enable the use of the <code>lightning</code> simulation backend by pennylane, which speeds up simulation </li> </ul> <p>We also add an observation wrapper called <code>ArctanNormalizationWrapper</code> at the very beginning of the file. Because we encode the features of the states as rotations, we need to ensure that the features are not beyond the interval of - \u03c0 and \u03c0 due to the periodicity of the rotation gates. For more details on wrappers, see Advanced Usage.</p>"},{"location":"algorithms/reinforce/#experiment-results","title":"Experiment results","text":""},{"location":"algorithms/reinforce/#continuous-state-continuous-action","title":"Continuous state - continuous action","text":"<p>The <code>reinforce_classical_continuous_action.py</code> and the <code>reinforce_quantum_continuous_action.py</code> have the following features:</p> <ul> <li>\u2705 Work with the continuous observation space </li> <li>\u2705 Work with the continuous action space</li> <li>\u2705 Work with envs like Pendulum-v1</li> <li>\u2705 Multiple Vectorized Environments </li> <li>\u2705 Single file implementation </li> </ul>"},{"location":"algorithms/reinforce/#implementation-details_1","title":"Implementation details","text":"<p>The implementations follow the same principles as <code>reinforce_classical.py</code> and <code>reinforce_quantum.py</code>. In the following we focus on the key differences between <code>reinforce_quantum_continuous_action.py</code> and <code>reinforce_quantum.py</code>. The same differences also apply for the classical implementations.</p> <p>While the state encoding is the same as for the previous approach, we need to implement some modifications in order to draw continuous actions with the parameterized quantum circuit. For that we modify the <code>ReinforceAgentQuantum</code> class as follows:</p> reinforce_quantum_continuous_action.py<pre><code>class ReinforceAgentQuantumContinuous(nn.Module):\n    def __init__(self, observation_size, num_actions, config):\n        super().__init__()\n        self.config = config\n        self.observation_size = observation_size\n        self.num_actions = num_actions\n        self.num_qubits = config[\"num_qubits\"]\n        self.num_layers = config[\"num_layers\"]\n\n        .....\n\n\n        # additional trainable parameters for the variance of the continuous actions\n        self.actor_logstd = nn.Parameter(torch.zeros(1, num_actions))\n\n    def get_action_and_logprob(self, x):\n        action_mean = self.quantum_circuit(\n            x,\n            self.input_scaling,\n            self.weights,\n            self.num_qubits,\n            self.num_layers,\n            self.num_actions,\n            self.observation_size,\n        )\n        action_mean = torch.stack(action_mean, dim=1)\n        action_mean = action_mean * self.output_scaling\n        action_logstd = self.actor_logstd.expand_as(action_mean)\n        action_std = torch.exp(action_logstd)\n        probs = torch.distributions.Normal(action_mean, action_std)\n        action = probs.sample()\n        return action, probs.log_prob(action)\n</code></pre> <p>In our implementation, the mean of the continuous action is based on the expectation value of the parameterized quantum circuit, while the variance is an additional classical trainable parameter. This parameter is also the same for all continuous actions. For additional information we refer to Variational Quantum Circuit Design for Quantum Reinforcement Learning on Continuous Environments</p>"},{"location":"algorithms/reinforce/#experiment-results_1","title":"Experiment results","text":""},{"location":"algorithms/reinforce/#discrete-state-discrete-action","title":"Discrete state - discrete action","text":"<p>The <code>reinforce_classical_discrete_state.py</code> and the <code>reinforce_quantum_discrete_state.py</code> have the following features:</p> <ul> <li>\u2705 Work with the discrete observation space </li> <li>\u2705 Work with the discrete action space</li> <li>\u2705 Work with envs like FrozenLake-v1</li> <li>\u2705 Multiple Vectorized Environments </li> <li>\u2705 Single file implementation </li> </ul>"},{"location":"algorithms/reinforce/#implementation-details_2","title":"Implementation details","text":"<p>The implementations follow the same principles as <code>reinforce_classical.py</code> and <code>reinforce_quantum.py</code>. In the following we focus on the key differences between <code>reinforce_quantum_discrete_state.py</code> and <code>reinforce_quantum.py</code>. The same differences also apply for the classical implementations.</p> <p>The key difference is the state encoding. Since discrete state environments like FrozenLake return an integer value, it is straight forward to encode the state as a binary value instead of an integer. For that, an additional function for <code>ReinforceAgentQuantum</code> is added called <code>encoding_input</code>. This converts the integer value into its binary value.</p> <p>reinforce_quantum_discrete_state.py<pre><code>    def encode_input(self, x):\n        x_binary = torch.zeros((x.shape[0], self.observation_size))\n        for i, val in enumerate(x):\n            binary = bin(int(val.item()))[2:]\n            padded = binary.zfill(self.observation_size)\n            x_binary[i] = torch.tensor([int(bit) * np.pi for bit in padded])\n        return x_binary\n</code></pre> Now we just need to also call this function before we pass the input to the parameterized quantum circuit:</p> reinforce_quantum_discrete_state.py<pre><code>    def get_action_and_logprob(self, x):\n        x_encoded = self.encode_input(x)\n        logits = self.quantum_circuit(\n            x_encoded,\n            self.input_scaling,\n            self.weights,\n            self.num_qubits,\n            self.num_layers,\n            self.num_actions,\n            self.observation_size\n        )\n        logits = torch.stack(logits, dim=1)\n        logits = logits * self.output_scaling\n        probs = Categorical(logits=logits)\n        action = probs.sample()\n        return action, probs.log_prob(action)\n</code></pre>"},{"location":"algorithms/reinforce/#experiment-results_2","title":"Experiment results","text":""},{"location":"algorithms/reinforce/#jumanji-environments","title":"Jumanji Environments","text":"<p>The <code>reinforce_classical_jumanji.py</code> and the <code>reinforce_quantum_jumanji.py</code> have the following features:</p> <ul> <li>\u2705 Work with jumanji environments </li> <li>\u2705 Work with envs like Traveling Salesperson and Knapsack</li> <li>\u2705 Multiple Vectorized Environments </li> <li>\u274c No single file implementation (require custom wrapper file for jumanji <code>jumanji_wrapper.py</code>)</li> </ul>"},{"location":"algorithms/reinforce/#implementation-details_3","title":"Implementation details","text":"<p>The implementations follow the same principles as <code>reinforce_classical.py</code> and <code>reinforce_quantum.py</code>. In the following we focus on the key differences between <code>reinforce_quantum_jumanji.py</code> and <code>reinforce_quantum.py</code>. The same differences also apply for the classical implementations.</p> <p>For most of the <code>`jumanji</code> environments, the observation space is quite complex. Instead of simple numpy arrays for the states, we often have dictionary states which vary in size and shape. E.g. the Knapsack problem returns a state of shape </p> <ul> <li><code>weights</code>: jax array (float) of shape (num_items,), array of weights of the items to be packed into the knapsack.</li> <li><code>values</code>: jax array (float) of shape (num_items,), array of values of the items to be packed into the knapsack.</li> <li><code>packed_items</code>: jax array (bool) of shape (num_items,), array of binary values denoting which items are already packed into the knapsack.</li> <li><code>action_mask</code>: jax array (bool) of shape (num_items,), array of binary values denoting which items can be packed into the knapsack.</li> </ul> <p>In order to parse this to a parameterized quantum circuit or a neural network, we can use a gym wrapper which converters the state again to an array. This is being done when calling the function <code>create_jumanji_wrapper</code>. For more details see Jumanji Wrapper. </p> reinforce_quantum_jumanji.py<pre><code>def make_env(env_id, config):\n    def thunk():\n        env = create_jumanji_env(env_id, config)\n\n        return env\n\n    return thunk\n</code></pre> reinforce_quantum_jumanji.py<pre><code>class ReinforceAgentQuantum(nn.Module):\n    def __init__(self, num_actions, config):\n        super().__init__()\n        self.config = config\n        self.num_actions = num_actions\n        self.num_qubits = config[\"num_qubits\"]\n        self.num_layers = config[\"num_layers\"]\n        self.block_size = 3  # number of subblocks depends on environment\n</code></pre> <p>Because the state is now growing quickly in size as the number of items of the Knapsack is increased, we use a slightly different approach to encode it: Instead of encoding each feature of the state on an individual qubit, we divide the state again into 3 blocks, namely <code>weights</code>, <code>values</code>and <code>packed_items</code>. This will be important for the modification of our ansatz for the parameterized quantum circuit which you can see below:</p> reinforce_quantum_jumanji.py<pre><code>def parametrized_quantum_circuit(\n    x, input_scaling, weights, num_qubits, num_layers, num_actions\n):\n\n    # This block needs to be adapted depending on the environment.\n    # The input vector is of shape [4*num_actions] for the Knapsack:\n    # [action mask, packed items, values, weights]\n\n    annotations = x[:, num_qubits : num_qubits * 2]\n    values_kp = x[:, num_qubits * 2 : num_qubits * 3]\n    weights_kp = x[:, 3*num_qubits:]\n\n    for layer in range(num_layers):\n        for block, features in enumerate([annotations, values_kp, weights_kp]):\n            for i in range(num_qubits):\n                qml.RX(input_scaling[layer, block, i] * features[:, i], wires=[i])\n\n            for i in range(num_qubits):\n                qml.RY(weights[layer, block, i], wires=[i])\n\n            for i in range(num_qubits):\n                qml.RZ(weights[layer, block, i+num_qubits], wires=[i])\n\n            if num_qubits == 2:\n                qml.CZ(wires=[0, 1])\n            else:\n                for i in range(num_qubits):\n                    qml.CZ(wires=[i, (i + 1) % num_qubits])\n\n    return [qml.expval(qml.PauliZ(wires=i)) for i in range(num_actions)]\n</code></pre> <p>We encode each of these blocks individually in each layer. By that, we can save quantum circuit width, so the number of qubits, by increasing our quantum circuit depth, so the number of gates we are using. However, this still is not an optimal encoding. See our Tutorials section for better ansatz design.</p>"},{"location":"algorithms/reinforce/#experiment-results_3","title":"Experiment results","text":""},{"location":"benchmarks/ddpg_benchmarks/","title":"DDPG","text":""},{"location":"benchmarks/dqn_benchmarks/","title":"DQN","text":""},{"location":"benchmarks/overview/","title":"Overview","text":"<p>Here we provide experimental information for all implemented algorithms. We've compiled a collection of interactive reports in Weights &amp; Biases to showcase our tracked Quantum Reinforcement Learning experiments. These reports allow researchers to easily investigate experiment details. This level of insight is rarely available in other (Q)RL evaluation frameworks.</p>"},{"location":"benchmarks/overview/#open-rl-benchmark","title":"Open RL Benchmark","text":"<p>Please also check out https://benchmark.cleanrl.dev/ for a collection of Weights and Biases reports showcasing a variety of other (Q)RL experiments. </p>"},{"location":"benchmarks/ppo_benchmarks/","title":"PPO","text":""},{"location":"benchmarks/reinforce_benchmarks/","title":"REINFORCE","text":""},{"location":"tutorials/knapsack/","title":"Knapsack","text":""},{"location":"tutorials/overview/","title":"Overview","text":"<p>In this section we provide a list of tutorials. We show you how to easily and quickly change specific parts of the code in order to enhance the QRL agents and apply them to various custom problems. </p> <ul> <li> <p>Maze Games: Why scaling is an issue</p> </li> <li> <p>Graph encodings for the Traveling Salesperson Problem (TSP)</p> <p>In this tutorial, we will show how to implement the ideas of Andrea Skolik's paper Equivariant quantum circuits for learning on weighted graphs</p> </li> <li> <p>Hamiltonian encodings for the Knapsack Problem (KP)</p> <p>In this tutorial, we will show how to use the results from the paper Hamiltonian-based Quantum Reinforcement Learning for Neural Combinatorial Optimization</p> </li> </ul>"},{"location":"tutorials/tsp/","title":"TSP Example","text":""}]}