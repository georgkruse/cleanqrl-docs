{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-cleanqrl","title":"Welcome to CleanQRL","text":"<p>CleanQRL is a Reinforcement Learning library specifically tailored to the subbranch of Quantum Reinforcement Learning and is greatly inspired by the amazing work of CleanRL. Just as the classical analogue, we aim to provide high-quality single-file implementation with research-friendly features. The implementation follows mainly the ideas of CleanRL and is clean and simple, yet can scale nicely trough additional features such as ray tune. The main features of this repository are</p> <ul> <li>\ud83d\udcdc Single-file implementations of classical and quantum version of 5+ Reinforcement Learning agents </li> <li>\ud83d\udcbe Tuned and Benchmarked agents (with available configs)</li> <li>\ud83c\udfae Integration of gymnasium, mujoco and jumanji</li> <li>\ud83d\udcd8 Examples on how to enhance the standard QRL agents on a variety of games</li> <li>\ud83d\udcc8 Tensorboard Logging</li> <li>\ud83c\udf31 Local Reproducibility via Seeding</li> <li>\ud83e\uddeb Experiment Management with Weights and Biases</li> <li>\ud83d\udcca Easy and straight forward hyperparameter tuning with ray tune</li> </ul> <p>What we are missing compared to CleanRL:</p> <ul> <li>\ud83d\udcb8 Cloud Integration with docker and AWS </li> <li>\ud83d\udcf9 Videos of Gameplay Capturing</li> </ul> <p>You can read more about CleanQRL in our upcoming paper.</p>"},{"location":"#contact-and-community","title":"Contact and Community","text":"<p>We want to grow as a community, so posting Github Issues and PRs are very welcome! If you are missing and algorithms or have a specific problem to which you want to tailor your QRL algorithms but fail to do so, you can also create a feature request!</p>"},{"location":"#citing-cleanqrl","title":"Citing CleanQRL","text":"<p>If you use CleanQRL in your work, please cite our [paper]:</p>"},{"location":"#citing-cleanrl","title":"Citing CleanRL","text":"<p>If you used mainly the classical parts of our code in your work, please cite the original CleanRL paper:</p> <pre><code>@article{huang2022cleanrl,\n  author  = {Shengyi Huang and Rousslan Fernand Julien Dossa and Chang Ye and Jeff Braga and Dipam Chakraborty and Kinal Mehta and Jo\u00e3o G.M. Ara\u00fajo},\n  title   = {CleanRL: High-quality Single-file Implementations of Deep Reinforcement Learning Algorithms},\n  journal = {Journal of Machine Learning Research},\n  year    = {2022},\n  volume  = {23},\n  number  = {274},\n  pages   = {1--18},\n  url     = {http://jmlr.org/papers/v23/21-1342.html}\n}\n</code></pre>"},{"location":"algorithms/","title":"Algorithms","text":""},{"location":"algorithms/#overview","title":"Overview","text":"Algorithm Variants Implemented \u2705 REINFORCE <code>reinforce_classical.py</code>, docs <code>reinforce_quantum.py</code>, docs \u2705 Deep Q-Learning (DQN) <code>dqn_classical.py</code>, docs <code>dqn_quantum.py</code>, docs \u2705 Proximal Policy Gradient (PPO) <code>ppo_classical.py</code>, docs <code>ppo_classical_continuous.py</code>, docs <code>ppo_classical_jumanji.py</code>, docs <code>ppo_quantum.py</code>, docs <code>ppo_quantum_continuous.py</code>, docs <code>ppo_quantum_jumanji.py</code>, docs \u2705 Deep Deterministic Policy Gradient (DDPG) <code>ddpg_classical.py</code>, docs <code>ddpg_quantum.py</code>, docs"},{"location":"algorithms/#reinforce","title":"REINFORCE","text":""},{"location":"algorithms/#reinforce_classicalpy","title":"<code>reinforce_classical.py</code>","text":"<p>The <code>reinforce_classical.py</code> has the following features:</p> <ul> <li>\u2705 Works with the Box observation space of low-level features</li> <li>\u2705 Works with the Discrete action space</li> <li>\u2705 Works with envs like CartPole-v1</li> <li>\u2705 Vectorized Environments </li> </ul>"},{"location":"algorithms/#implementation-details","title":"Implementation details","text":""},{"location":"algorithms/#experiment-results","title":"Experiment results","text":""},{"location":"algorithms/#reinforce_quantumpy","title":"<code>reinforce_quantum.py</code>","text":"<p>The <code>reinforce_quantum.py</code> has the following features:</p> <ul> <li>\u2705 Works with the Box observation space of low-level features</li> <li>\u2705 Works with the Discrete action space</li> <li>\u2705 Works with envs like CartPole-v1</li> <li>\u2705 Vectorized Environments </li> </ul>"},{"location":"algorithms/#implementation-details_1","title":"Implementation details","text":""},{"location":"algorithms/#experiment-results_1","title":"Experiment results","text":""},{"location":"algorithms/#dqn","title":"DQN","text":""},{"location":"algorithms/#dqn_classicalpy","title":"<code>dqn_classical.py</code>","text":"<p>The <code>dqn_classical.py</code> has the following features:</p> <ul> <li>\u2705 Works with the Box observation space of low-level features</li> <li>\u2705 Works with the Discrete action space</li> <li>\u2705 Works with envs like CartPole-v1</li> <li>\u274c Vectorized Environments not enabled</li> <li>\u274c Requieres <code>replay_buffer.py</code> (no single file implementation)</li> </ul>"},{"location":"algorithms/#implementation-details_2","title":"Implementation details","text":""},{"location":"algorithms/#experiment-results_2","title":"Experiment results","text":""},{"location":"algorithms/#dqn_quantumpy","title":"<code>dqn_quantum.py</code>","text":"<p>The <code>dqn_quantum.py</code> has the following features:</p> <ul> <li>\u2705 Works with the Box observation space of low-level features</li> <li>\u2705 Works with the Discrete action space</li> <li>\u2705 Works with envs like CartPole-v1</li> <li>\u274c Vectorized Environments not enabled</li> <li>\u274c Requieres <code>replay_buffer.py</code> (no single file implementation)</li> </ul>"},{"location":"algorithms/#implementation-details_3","title":"Implementation details","text":""},{"location":"algorithms/#experiment-results_3","title":"Experiment results","text":""},{"location":"get_started/","title":"Get Started","text":""},{"location":"get_started/#installation","title":"Installation","text":"<p>To run experiments locally, you need to clone the repository and install a python environment.</p> <pre><code>git clone https://github.com/georgkruse/cleanqrl.git\ncd cleanqrl\nconda env create -f environment.yaml\n</code></pre> <p>Thats it, now you set up!</p>"},{"location":"get_started/#run-first-experiments","title":"Run first experiments","text":"<p>Each agent can be run as a single file, either from the parent directory or directly in the subfolder. First, activate the environment <code>cleanqrl</code> and then execute the algorithm's python file:</p> <pre><code>conda activate cleanqrl\npython cleanrl/reinforce_quantum.py \n</code></pre> <p>or go directly into the folder and execute</p> <pre><code>conda activate cleanqrl\ncd cleanqrl \npython reinforce_quantum.py \n</code></pre> <p>Before you execute the files, customize the parameters in the  <code>Config</code> class at the end of each file. Every file has such a dataclass object and the algorithm is callable as a function which takes the config as input:</p> <pre><code>def reinforce_quantum(config):\n    num_envs = config[\"num_envs\"]\n    total_timesteps = config[\"total_timesteps\"]\n    env_id = config[\"env_id\"]\n    gamma = config[\"gamma\"]\n    lr_input_scaling = config[\"lr_input_scaling\"]\n    lr_weights = config[\"lr_weights\"]\n    lr_output_scaling = config[\"lr_output_scaling\"]\n    .... \n</code></pre> <p>This function can also be called from an external file (see below for details). But first, lets have a closer look to the the <code>Config</code>: </p> reinforce_quantum.py<pre><code>@dataclass\nclass Config:\n    # General parameters\n    trial_name: str = 'reinforce_quantum'  # Name of the trial\n    trial_path: str = 'logs'  # Path to save logs relative to the parent directory\n    wandb: bool = False # Use wandb to log experiment data \n    project_name: str = \"cleanqrl\"  # If wandb is used, name of the wandb-project\n\n    # Environment parameters\n    env_id: str = \"CartPole-v1\" # Environment ID\n\n    # Algorithm parameters\n    num_envs: int = 1  # Number of environments\n    total_timesteps: int = 100000  # Total number of timesteps\n    gamma: float = 0.99  # discount factor\n    lr_input_scaling: float = 0.01  # Learning rate for input scaling\n    lr_weights: float = 0.01  # Learning rate for variational parameters\n    lr_output_scaling: float = 0.01  # Learning rate for output scaling\n    cuda: bool = False  # Whether to use CUDA\n    num_qubits: int = 4  # Number of qubits\n    num_layers: int = 2  # Number of layers in the quantum circuit\n    device: str = \"default.qubit\"  # Quantum device\n    diff_method: str = \"backprop\"  # Differentiation method\n    save_model: bool = True # Save the model after the run\n</code></pre> <p>As you can see, the config is devided into 3 parts:</p> <ul> <li>General parameters: Here the name of your experiment as well as the logging path is defined. All metrics will be logged in a <code>result.json</code> file in the result folder which will have the time of the experiment execution as a prefix. You can also use wandb for enhanced metric logging. </li> <li>Environment parameters: This is in the simplest case just the string of the gym environment. For jumanji environments as well as for your custom environments, you can also specify additional parameters here (see #Tutorials for details).</li> <li>Algorithms parameters: All algorithms hyperparameters are specified here. For details on the parameters see the algorithms section</li> </ul> <p>Once you execute the file, it will create the subfolders and copy the config which is used for the experiment in the folder:</p> reinforce_quantum.py<pre><code>    config = vars(Config())\n\n    # Based on the current time, create a unique name for the experiment\n    config['trial_name'] = datetime.now().strftime(\"%Y-%m-%d--%H-%M-%S\") + '_' + config['trial_name']\n    config['path'] = os.path.join(Path(__file__).parent.parent, config['trial_path'], config['trial_name'])\n\n    # Create the directory and save a copy of the config file so that the experiment can be replicated\n    os.makedirs(os.path.dirname(config['path'] + '/'), exist_ok=True)\n    config_path = os.path.join(config['path'], 'config.yml')\n    with open(config_path, 'w') as file:\n        yaml.dump(config, file)\n\n    # Start the agent training \n    reinforce_quantum(config)   \n</code></pre> <p>After the execution, the experiment data is saved e.g. at: </p> <pre><code>...\nconfigs\nexamples\nlogs/\n    2025-03-04--14-59-32_reinforce_quantum          # The name of your experiment\n        config.yaml                                 # Config which was used to run this experiment\n        result.json                                 # Results of the experiment\n.gitignore\n...\n</code></pre> <p>You can also set the <code>wandb</code> variable to <code>True</code>:</p> reinforce_quantum.py<pre><code>@dataclass\nclass Config:\n    # General parameters\n    trial_name: str = 'reinforce_quantum_wandb'  # Name of the trial\n    trial_path: str = 'logs'  # Path to save logs relative to the parent directory\n    wandb: bool = True # Use wandb to log experiment data \n    project_name: str = \"cleanqrl\"  # If wandb is used, name of the wandb-project\n\n    # Environment parameters\n    env_id: str = \"CartPole-v1\" # Environment ID\n</code></pre> <p>You will need to login to your wandb account before you can run:</p> <pre><code>wandb login # only required for the first time\npython cleanrl/reinforce_quantum.py \\\n</code></pre> <p>This will create an additional folder for the wandb logging and you can inspect your experiment data also online:</p> <pre><code>...\nconfigs\nexamples\nlogs/\n    2025-03-04--14-59-32_reinforce_quantum_wandb    # The name of your experiment\n        wandb                                       # Subfolder of the wandb logging\n        config.yaml                                 # Config which was used to run this experiment\n        result.json                                 # Results of the experiment\n.gitignore\n...\n</code></pre>"},{"location":"get_started/#run-experiments-with-config-files","title":"Run experiments with config files","text":"<p>Additionally, all algorithms can be executed from an external script as well. There are two examples in the root directory <code>main.py</code> and <code>main_batch.py</code>. You can specify all parameters in a YAML file instead (and also reuse the <code>config.yaml</code>files which have been generated in previous runs). For examples, take a look at the <code>configs/basic</code>folder. You will just need to specify the config path. </p> main.py<pre><code>    config_path = \"configs/basic/reinforce_quantum.yaml\"\n\n    # Load the config file\n    with open(config_path) as f:\n        config = yaml.load(f, Loader=yaml.FullLoader)\n\n    # Based on the current time, create a unique name for the experiment\n    config[\"trial_name\"] = (\n        datetime.now().strftime(\"%Y-%m-%d--%H-%M-%S\") + \"_\" + config[\"trial_name\"]\n    )\n    config[\"path\"] = os.path.join(\n        os.path.dirname(os.getcwd()), config[\"trial_path\"], config[\"trial_name\"]\n    )\n\n    # Create the directory and save a copy of the config file so\n    # that the experiment can be replicated\n    os.makedirs(os.path.dirname(config[\"path\"] + \"/\"), exist_ok=True)\n    shutil.copy(config_path, os.path.join(config[\"path\"], \"config.yaml\"))\n\n    # Start the agent training\n    train_agent(config)\n</code></pre> <p>If you want to execute several config files sequentially, you can also you the <code>main_batch.py</code> file, where you can specify several configs in a list or execute all configs in a subdirectory.</p> main_batch.py<pre><code>    # Specify the path to the config file\n    # Get all config files in the configs folder\n    # config_files = [f for f in os.listdir('configs/basic') if f.endswith('.yaml')]\n    config_paths = [\n        \"configs/basic/dqn_classical.yaml\",\n        \"configs/basic/reinforce_classical.yaml\",\n        \"configs/basic/reinforce_classical_continuous_action.yaml\",\n        \"configs/basic/ppo_classical.yaml\",\n        \"configs/basic/ppo_classical_continuous_action.yaml\",\n    ]\n</code></pre>"},{"location":"get_started/#experiment-logging","title":"Experiment logging","text":"<p>By default, all metrics are logged to a <code>result.json</code> file on the experiment folder. Plots are generated by default for these runs as well for some of the metrics. For details, take a look at <code>cleanqrl_utils/plotting.py</code>. </p> <p>When using <code>wandb</code>, all data is additionally logged to your wandb account .</p>"},{"location":"advanced_usage/hyperparameter_tuning/","title":"Hyperparameter Tuning","text":""},{"location":"advanced_usage/hyperparameter_tuning/#ray-tune","title":"Ray Tune","text":"<p>Generally, in (Q)RL, hyperparameter tuning is essential. Therefore, we offer easy hyperparamter tunings with ray tune. To use it, you can take a look at the config files located in the <code>configs/tune</code> folder and the <code>tune.py</code>.</p> tune.py<pre><code>    # Generate the parameter space for the experiment from the config file\n    config = add_hyperparameters(config)\n\n    .... \n\n    # Instead of running a single agent as before, we will use ray.tune to run multiple agents\n    # in parallel. We will use the same train_agent function as before.\n    ray.init(\n        local_mode=config[\"ray_local_mode\"],\n        num_cpus=config[\"num_cpus\"],\n        num_gpus=config[\"num_gpus\"],\n        _temp_dir=os.path.join(os.path.dirname(os.getcwd()), \"t\"),\n        include_dashboard=False,\n    )\n\n    # We need an addtional function to create subfolders for each hyperparameter configuration\n    def trial_name_creator(trial):\n        return trial.__str__() + \"_\" + trial.experiment_tag\n\n    # We will use the tune.Tuner class to run multiple agents in parallel\n    trainable = tune.with_resources(\n        train_agent,\n        resources={\"cpu\": config[\"cpus_per_worker\"], \"gpu\": config[\"gpus_per_worker\"]},\n    )\n    tuner = tune.Tuner(\n        trainable,\n        param_space=config,\n        run_config=tune.RunConfig(storage_path=config[\"path\"]),\n        tune_config=tune.TuneConfig(\n            num_samples=config[\"num_samples\"],\n            trial_dirname_creator=trial_name_creator,\n        ),\n    )\n\n    # The fit function will start the hyperparameter search\n    tiral = tuner.fit()\n</code></pre> <p>We use the tune.Tuner to perform the hyperparameter search. But before we take a look at the hyperparameters, lets see first how to define the resources: In the config files, we have an additional block:</p> <p><pre><code># ray tune parameters\nray_local_mode:         False\nnum_cpus:               24\nnum_gpus:               0\nnum_samples:  3\ncpus_per_worker:        1\ngpus_per_worker:        0\n</code></pre> The first parameter defines if we want to use the so called <code>local_mode</code> which enforces sequential execution instead of parallel execution for debugging purposes. Hence, this should always be False if you want to run the actual training. Then you need to specify the amount of cpus and gpus you want to make available to the tune.Tuner. This depends on the machine you are using. Next, you need to define the number of samples you want to run for each hyperparameter configuration. Generally, you do not want the number to be too small, because especially QRL experiments can have large variances in performance. But also you don't want this number to be too big, because this will cause very long runtimes. Lastly, you need to define how many resources each worker, that is each sample, gets to run. E.g. If you specify <code>num_cpus=10</code> and <code>cpus_per_worker=1</code>, then 10 runs will be run in parallel.</p>"},{"location":"advanced_usage/hyperparameter_tuning/#add-hyperparameters","title":"Add hyperparameters","text":"<p>Adding hyperparameters to sample is straight forward. For that, you can look at the following example. Instead of specifying e.g. </p> <pre><code>lr_weights: 0.001 \n</code></pre> <p>You can instead specify it as:</p> <pre><code>lr: \n    - grid_search           # Do a grid search with number of seeds == num_samples\n    - float                 # Define the type [float, int, str, list]\n    - [0.01, 0.001, 0.0001] # List of parameters to select\n</code></pre> <p>If <code>num_samples</code> is set to 5 and <code>num_cpus</code> to 15, then each hyperparameter configuration will be run with 5 seeds in parallel (watch your RAM for larger qubit numbers).</p> <p>You can now also edit other variables:</p> <pre><code>batch_size: \n    - grid_search           # Do a grid search with number of seeds == num_samples\n    - int                   # Define the type [float, int, str, list]\n    - [16, 32]              # List of parameters to select\n</code></pre> <p>This will now start for each of the <code>batch_size</code> a hyperparameter run with all the specified learning rates for <code>num_samples</code>, so a total of 30 trials. If <code>num_cpus</code> is set to 10, then it will sequentially execute the 30 trials with 10 trials in parallel.</p> <p>You can even do a search over the environments as:</p> <pre><code>env_id: \n    - grid_search           # Do a grid search with number of seeds == num_samples\n    - str                   # Define the type [float, int, str, list]\n    - ['CartPole-v1',       # List of parameters to select\n       'Acrobot-v1']              \n</code></pre> <p>Finally, just like for the <code>main.py</code>, there exists also a <code>tune_batch.py</code> file, where you can sequentially perform hyperparameter runs.</p>"},{"location":"advanced_usage/jumanji_environments/","title":"Jumanji Environments","text":""},{"location":"advanced_usage/jumanji_environments/#classical-agents","title":"Classical agents","text":""},{"location":"advanced_usage/jumanji_environments/#quantum-agents","title":"Quantum agents","text":""},{"location":"advanced_usage/jumanji_environments/#jumanji-wrappers","title":"Jumanji wrappers","text":""},{"location":"benchmarks/ddpg_benchmarks/","title":"DDPG","text":""},{"location":"benchmarks/dqn_benchmarks/","title":"DQN","text":""},{"location":"benchmarks/overview/","title":"Overview","text":"<p>Here we provide experimental information for all implemented algorithms. We've compiled a collection of interactive reports in Weights &amp; Biases to showcase our tracked Quantum Reinforcement Learning experiments. These reports allow researchers to easily investigate experiment details. This level of insight is rarely available in other (Q)RL evaluation frameworks.</p>"},{"location":"benchmarks/overview/#open-rl-benchmark","title":"Open RL Benchmark","text":"<p>Please also check out https://benchmark.cleanrl.dev/ for a collection of Weights and Biases reports showcasing a variety of other (Q)RL experiments. </p>"},{"location":"benchmarks/ppo_benchmarks/","title":"PPO","text":""},{"location":"benchmarks/reinforce_benchmarks/","title":"REINFORCE","text":""},{"location":"tutorials/knapsack/","title":"Knapsack","text":""},{"location":"tutorials/overview/","title":"Overview","text":"<p>In this section we provide a list of tutorials. We show you how to easily and quickly change specific parts of the code in order to enhance the QRL agents and apply them to various custom problems. </p> <ul> <li> <p>Maze Games: Why scaling is an issue</p> </li> <li> <p>Graph encodings for the Traveling Salesperson Problem (TSP)</p> <p>In this tutorial, we will show how to implement the ideas of Andrea Skolik's paper Equivariant quantum circuits for learning on weighted graphs</p> </li> <li> <p>Hamiltonian encodings for the Knapsack Problem (KP)</p> <p>In this tutorial, we will show how to use the results from the paper Hamiltonian-based Quantum Reinforcement Learning for Neural Combinatorial Optimization</p> </li> </ul>"},{"location":"tutorials/tsp/","title":"TSP Example","text":""}]}